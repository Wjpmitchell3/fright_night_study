---
title: "ER Choice v. Usage - Study_1"
author: "Mitchell, W.J."
date: "`r Sys.Date()`"
output: html_document
---

NAVIGATION / QUESTIONS:
+ Lab research suggests that there are few stronger predictors of how a person might choose to regulate their emotions than affective intensity. Gal Sheppes and his lab have some really brilliant research on ER Choice that has laid the groundwork for the field. However, many of these studies create a sample that differs significantly from the general population in order to study the phenomena in isolation. Stimuli are dynamic and multimodal, rather than static and unimodal. Emotions evolve over a timecourse rather than in an instant. We often don't get to prepare for how we feel. We also don't have training and instructions on when, why, and how to regulate our emotions. We also aren't often limited to a single regulatory choice. EMA studies certainly get at this, but often fail to capture significant variation in emotive experiences. As such, choice differs significantly from usage. What we wanted to explore with this data was whether this bears true in the data. In the messiness of the real world, will affective intensity continue to predict which strategies people use? 

PREDICTIONS:
+ We do not believe that negative affective intensity will continue to demonstrate predictive utility towards ER usage (i.e., distraction v. reappraisal) when measured beyond a lab context. We will adjust for a number of individual difference measures in the process.

ISSUES:

NOTES:

# Setup
```{r Packages I Use}
  # Loading in my packages with my pacman manager
  pacman::p_load(effects,  
                 lme4, 
                 performance, 
                 stargazer, 
                 tidyverse)
```

``` {r Number Formatting}
  # Giving some commands for how numbers appear in R
  options(scipen=100)
  options(digits=3)
  options(tinytex.verbose = TRUE)
```

```{r Setting Working Directories}
  # Identifying our main directory
  WorkDir <- "C:/Users/wjpmi/Dropbox/My PC (UncleSplashysSaddnessEmporium)/Desktop/Grad School/Projects/Complete/Fright Night/"

  # Identifying specific directories to read and write from
  Export <- paste0(WorkDir, "Public_Records/data/Study_01/Derivative/")
  Import <- paste0(WorkDir, "Public_Records/data/Study_01/Raw/")
  Plots <- paste0(WorkDir, "Public_Records/plots/")
```

## Building the Dataframe

We're going to start by loading in our cleaned dataframe
```{r Loading dataframe}
  # Loading in our cleaned dataset
  setwd(Import)
  df <- read.csv(file = "df.csv", 
                 header = T, sep=",", 
                 stringsAsFactors = F,
                 na.strings=c("","N/A"), 
                 row.names = 1)
```

### Removing Excluded Participants

Participant 1339 had reported being at the haunted house the week prior to exposure, so they will also be removed. 1307 and 1308 were removed for concerns regarding english fluency.
```{r Subsetting Participants Beyond Exclusion Criteria}  
  # Removing participant who violated eligibility standards
  df <- subset(df,
               (df$PID != "1339" & 
                df$PID != "1307" & 
                df$PID != "1308"))
```

### General Data Cleaning / Correction

An executive decision prior to collection was that we'd collect fear intensity ratings for all reported events, independent of whether the participant indicated having felt fear in the first place. As a result, I don't feel comfortable using those ratings for analysis, so I am removing any instance of a fear rating that is not paired with an endorsement. 
```{r Removing Intensities Lacking Endorsement}
  # Removing ratings provided for emotions that were not endorsed.
  df$Emo.Extent[is.na(df$Presence)] <- NA
  df$Reg.Extent[is.na(df$Presence)] <- NA
  df$Reg.Extent[df$Reg.Extent == 1] <- NA
```  

### Cronbach's Alpha
Before we do anything, I want to assess the internal consistency of the emotions we are deeming to be negatively valenced. Given that we used a rather heavily modified measure, we need to complete some sort of a quality assurance measure. Face validity isn't quite enough.
```{r Building an Empty Dataframe to Calculate Chronbach's Alpha}
  # Generating an empty dataframe in a format conducive to completing the cronbach's alpha
  alpha <- data.frame(PID = sort(rep(unique(df$PID), 10)), 
                      Event = rep(1:10,47), 
                      Alert = NA,
                      Amuse = NA,
                      Excite = NA,
                      Aggress = NA,
                      Annoy = NA,
                      Upset = NA,
                      Nerv = NA,
                      Gross = NA,
                      Whelm = NA, 
                      Panic = NA, 
                      Tense = NA, 
                      Shock = NA, 
                      Fear = NA)
```                      
                      
```{r Building the dataframe by which to calculate Chronbach's Alpha}
# Populating that dataframe with data from the original dataframe
for (row in 1:nrow(df)){
  if (row %% 14 != 0) {
    if (!is.na(df$Presence[row])) {
      if (df$Presence[row] == 1) {
        alpha[ceiling(row/14), (2 + row %% 14)] <- df$Emo.Extent[row]
      }
    }
  }
}
```

```{r Calculating Chronbach's Alpha}
# Calculating Cronbach's Alpha for the negative emotions 
# NOTE: We did not have enough endorsements of feeling anger/aggression to include in the calculation
psych::alpha(alpha[,7:15], 
             na.rm = T, 
             use = "pairwise")

rm(alpha, row)
```

### Cronbach's Alpha Results

The Cronbach's alpha analysis indicated an internal consistency value of alpha = 0.91 [95% CI: 0.90 - 0.93], suggesting excellent internal consistency within our negatively valenced emotion construct.

### Creating New Variables 

Much to my chagrin, the existing event variable notes the order in which events were recalled, not the chronological order in which the events occurred. Those are captured in the "EventsOrdered" columns. It's confusing to wrap your head around but the number that exists in a cell in those columns represents which event, in recall order, occurred first chronologically. So if df$EventsOrdered[1] read "7", then that would mean that the 7th event that that participant recalled occurred first chronologically. We care about both, so I'm going to create an order column for both.

```{r Creating a variable tracking the order in which events were entered}
# Creating an empty variable in the dataframe
df$Event_chronolorder <- NA

# Iterating through each row ...
for (row in 1:nrow(df)){
  # and each column of events ...
  for (col in grep(x = names(df),
                 pattern = "EventsOrdered\\.*")){
    # and recording the appropriate index for each event.
    if (df$Event[row] == df[row,col]){
      df$Event_chronolorder[row] <- col + 1 - grep(x = names(df),
                                                   pattern = "EventsOrdered\\.2")
    }
  }
}

rm(row, col)
```

We are centering the intercept of the events on an extrapolated middle event (5.5). I struggled with whether to center it in the middle or on the first event. My logic is that we likely will see effects of time and I'm more interested in the average, which event 5.5 better represents and which might better represent the data as a whole, than the first event. 

```{r Making the intercept of event the first event}
  df$Event_recallorder <- as.numeric(df$Event - 4.5)
  df$Event_chronolorder <- as.numeric(df$Event_chronolorder - 3.5)
```

Reordering and subsetting the variables that we care about.

```{r Subsetting Relevant Data}
  df <- subset(df,
               !is.na(df$Emo.Extent),
               select = names(df)[c(1:2, 203:204, 6:11, 14, 16, 35:36, 39, 47,
                                    137:144, 163:167, 169:173, 188:197)])
```

Emotions and their regulation can be an extremely personal and idiosyncratic experience. As such, we expect that Person A's regulation might look different than Person B's and using person-centered multilevel models might better capture that. 

```{r Person Centering and Aggregating}   
df <- dplyr::group_by(df, as.factor(PID))
df <- dplyr::mutate(df, 
                 Emo.Extent.pmean = mean(Emo.Extent, na.rm=T),
                 Reg.Extent.pmean = mean(Reg.Extent, na.rm=T),
                 Emo.Extent.pc = Emo.Extent - mean(Emo.Extent, na.rm=T),
                 Reg.Extent.pc = Reg.Extent - mean(Reg.Extent, na.rm=T))
df <- ungroup(df)
```

Lastly, we're standardizing our variables so as to make them translatable and communicable.

```{r Z Scoring Aggregates}   
df <- group_by(df, PID)
df$Emo.Extent.pmean.z <- as.numeric(scale(df$Emo.Extent.pmean, scale = T, center = T))
df$Reg.Extent.pmean.z <- as.numeric(scale(df$Reg.Extent.pmean, scale = T, center = T))
df <- ungroup(df)
```

```{r Z Scoring Variables}
  df$Emo.Extent.z <- as.numeric(scale(df$Emo.Extent, scale=T, center=T))
  df$Reg.Extent.z <- as.numeric(scale(df$Reg.Extent, scale=T, center=T))
  df$Emo.Extent.pc.z <- as.numeric(scale(df$Emo.Extent.pc, scale=T, center=T))
  df$Reg.Extent.pc.z <- as.numeric(scale(df$Reg.Extent.pc, scale=T, center=T))
```

### Assessing the structure of our dataframe

Assessing whether the variables we want to use are structured correctly

```{r Structure of the dataframe}
  str(df)
```

Coercing the data that is misstructured to the correct structure

```{r Restructuing Variables}
  df$Emotion <- as.factor(df$Emotion)
  df$Valence <- as.factor(df$Valence)
  df$PID <- as.factor(df$PID)
  df$HauntedHouse_Times <- as.numeric(df$HauntedHouse_Times)
  df$HauntedHouse_Compare <- as.numeric(df$HauntedHouse_Compare)
```

```{r Adding Regulated Factor}  
  df$Regulated <- NA
  df$Regulated[df$Reg.Extent > 1] <- "Regulated"
  df$Regulated[df$Reg.Extent < 2 | is.na(df$Reg.Extent)] <- "Not Regulated"
  df$Regulated <- as.factor(df$Regulated)
```

## Calculating Quality Assurance Values and Exploring the Dataset

We now want to subset events that were not regulated, that did not have negative emotions, that were regulated with only reappraisal or distraction. We'll get some output as to the stats of this dataset as well.

```{r Primary Dataframe}
df <- subset(df, df$Regulated == "Regulated" & df$Valence == "Negative" & df$MultipleStrategies == 0 & ((df$Reappraisal == 1 & df$Attention.Deployment == 0) |
                 (df$Reappraisal == 0 & df$Attention.Deployment == 1)))
paste("Observations:", nrow(df))
paste("Unique Events:", nrow(unique(df[,c('PID', 'Event_chronolorder')])))
paste("Average Number of Emotions Per Event:", round(nrow(df)/nrow(unique(df[,c('PID', 'Event_chronolorder')])),2))
paste("Total Number of Participants:", nrow(distinct(subset(df, select = c("PID")))))
paste("Total Number of Reappraisal Events:", nrow(distinct(subset(df, df$Reappraisal == 1 , select = c("Reappraisal", "Event.ID")))))
paste("Total Number of Distraction Events:", nrow(distinct(subset(df, df$Attention.Deployment == 1 ,select = c("Attention.Deployment", "Event.ID")))))
```

I want to save this dataframe for future use as well. We're going to subset out variables irrelevant to our analyses.

```{r Saving Dataframe}
# Defining which columns we want 
cols_old <- c("Event.ID", "Event_chronolorder", "Attention.Deployment", "Emotion", "Emo.Extent", "Emo.Extent.z", "Emo.Extent.pc.z",
              "Reg.Extent", "Reg.Extent.z", "Reg.Extent.pc.z", "Emo.Extent.pmean.z",
              "Reg.Extent.pmean.z", "PID", "ERQ_CogReapp", "ERQ_ExpSuppress", "IUS_Factor2", "BDI")

# Defining what we want to change their names to
cols_new <- c("Event", "Event_chronolorder", "Strategy.OG", "Emotion", "Emo.Extent", "Emo.Extent.z", "Emo.Extent.pc.z",
              "Reg.Extent", "Reg.Extent.z", "Reg.Extent.pc.z", "Emo.Extent.pmean.z",
              "Reg.Extent.pmean.z", "PID.OG", "ERQ_Reapp.OG", "ERQ_Supp.OG", "IUS_F2.OG", 
              "BDI")

# Subsettting those columns
df_temp <- subset(df, select = cols_old)

# Converting the names of reappraisal and disctration to more descriptive labels
df_temp$Attention.Deployment[df_temp$Attention.Deployment == 1] <- "Distraction"
df_temp$Attention.Deployment[df_temp$Attention.Deployment == 0] <- "Reappraisal"

# Changing the header names to what we want
names(df_temp) <- cols_new

# Saving that subset as a .csv
write.csv(df_temp, paste0(Export,"df_subset.csv"))
```

```{r Assessing the distributions of emotions}
as.data.frame(sort(table(df$Emotion),decreasing = T))
```

We see a wide range of emotion report frequencies; as few as 1 instance (Disgusted) and as many as 33 (Fear) among negative values. 

```{r Calculating the mean and sd for emotion frequency}
mean(table(as.character(df$Emotion)))
sd(table(as.character(df$Emotion)))
```

The mean number of times a negative emotion was reported was 16.6 instances with a standard deviation of 11.3. 

```{r One-Sample T Tests for Negative Valence}
  t.test(df$Emo.Extent, mu = 5, alternative = "two.sided")
  sd(df$Emo.Extent)
```

Our haunted house stimuli elicited sufficient affective intensity (x = 5.55, 95% CI = (5.37, 5.73), sd = 1.18, t(165) = 50, p < 0.001). N.B.: mu was equated to a value of 1 in both t-tests, as 1 was the lowest value on the affective intensity scale and referred to "None at all".

```{r Assessing the distributions of emotions}
plot <- ggplot(data = df, aes(x = Emotion, y = Emo.Extent)) + 
       theme_classic()+
       geom_jitter(aes(alpha=0.5, color=Emotion),shape=16, position=position_jitter(0.2)) + 
       geom_violin(trim=T, alpha=0.25) +
        labs(title = "Emotions Were Sufficiently Elicited Within Haunted House",
                    subtitle = expression(paste("t(165) = 50 ***")),
                     x = NULL, 
                     y ="Emotion Intensity") +
       scale_y_continuous(breaks = c(2:7)) +
       scale_color_nord(palette = "afternoon_prarie", discrete = T) +
       coord_cartesian(ylim=c(1.5,7.5)) +
       theme(legend.position="none") +
       geom_boxplot(width=0.2, color="black", alpha=0.0) +
       theme(plot.title = element_text(face="bold", size=8, hjust = 0.5)) +
       theme(plot.subtitle = element_text(size = 6, hjust = 0.5, face = "italic")) +
       theme(plot.caption = element_text(size = 8, hjust = 0.0, face = "italic")) +
       theme(axis.title = element_text(size = 14)) +
       theme(axis.text.x = element_text(size = 10, color = "Black", angle=-45, vjust = 1, hjust = 0.0)) +
       theme(axis.text.y = element_text(size = 12, color = "Black")) 
plot
```

```{r Exporting Emotion Means}
  setwd(Plots)
  pdf("FN01_EmoExtent_mean.pdf",
      width = 4, 
      height = 3)
  plot
  dev.off()
  rm(plot)
```

```{r Assessing the distributions of negative affect}
ggplot(data = df, aes(x = Valence, y = Emo.Extent)) + 
       theme_classic()+
       geom_jitter(aes(alpha=0.2),shape=16, position=position_jitter(0.2)) + 
       geom_violin(trim=T, alpha=0.5) +
       ylab("Affective Intensity") +
       theme(legend.position="none") +
       geom_boxplot(width=0.2, color="black", alpha=0.2)
```

# Preliminary Analyses

We're going to start with a bit of a sanity check: if we're tapping into regulation, it would make sense that as negative affective intensity increases, the effort to regulate that emotion increases as well. 

``` {r Regression Model}
m1 <- lm(Reg.Extent.z ~ Emo.Extent.z, data = df)
summary(m1)
```

``` {r Confidence Intervals for m1}
confint(m1, oldNames = FALSE)
```

When we examine whether there's a relationship between affective intensity and the extent to which it's regulated with a bivariate linear regression, we find that affective intensity does have predictive towards regulation extent [t(164) = 4.39, p < 0.001, B = 0.351, 95% CI(0.193, 0.509), adj.r < 0.100], such that as affective intensity increases, regulatory extent increases as well. 

```{r Regression Model Plots}
ggplot(data = df, aes(x = Emo.Extent.z, y = Reg.Extent.z, color = Emo.Extent.z)) +
    stat_smooth(aes(color = ..x..), method="lm", alpha = .25, size = 2) +
    labs(title = "Regulation Extent By Intensity of Emotion",
       subtitle = "Affective Intensity Predicts Regulation Extent (B = 0.351)",
       x = "Intensity of Emotion (z)", 
       y ="Regulation Extent (z)",
       caption = "p < 0.001") +
    coord_cartesian(xlim=c(-2.5, 1.5), ylim=c(-2.5, 2.5)) +
    scale_color_gradient2(low = "#1B9E77", high = "#D95F02", midpoint = median(-1)) +
    theme_classic() +
    theme(legend.position="none") +
    theme(plot.title = element_text(face="bold", size=13, hjust = 0.5)) +
    theme(plot.subtitle = element_text(size = 10, hjust = 0.5, face = "italic")) +
    theme(plot.caption = element_text(size = 8, hjust = 0.0, face = "italic")) +
    theme(axis.title = element_text(size = 16)) +
    theme(axis.text.x = element_text(size = 14, color = "Black")) +
    theme(axis.text.y = element_text(size = 14, color = "Black")) 
```

Great, it works out with a basic linear model. Let's check with a multilevel model that's person-centered (and maybe with some covariates).

```{r Null Model}
  m0 <- lmer(Reg.Extent.z ~ 1 + (1 | PID), data = df, REML = F)
  icc(m0)
``` 

```{r Experimental Models}
  m1 <- lmer(Reg.Extent.z ~ Emo.Extent.pc.z + Emo.Extent.pmean.z + (1 | PID), data = df, REML = F)  
  m2 <- lmer(Reg.Extent.z ~ Emo.Extent.pc.z + Emo.Extent.pmean.z + Age + Gender + BDI + 
             IUS_Factor2 + STAI_State + STAI_Trait + (1 | PID), data = df, REML = F)
```

```{r Model Comparisons}  
anova(m0, m1)
anova(m1, m2)
```

```{r Model Outputs}  
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
stargazer(m0,m1,m2,
          type = "text", 
          title= "Affect Predicting Regulation", 
          column.labels = c("Null Model", "Model 1", "Model 2"),
          dep.var.labels = "Regulation Extent (z)", 
          star.cutoffs = c(0.05, 0.01,0.001),
          covariate.labels = c("Affective Intensity (Person-Centered, z)", "Affective Intensity (Person-Mean)", "Age", "Gender",
                               "Depression", "IUS, Factor 2", "STAI, State", "STAI, Trait", "Intercept"),
          ci = TRUE,
          notes = "")
```

```{r Multilevel Model 1 Coefficients}
  coef(m2)
```  

```{r Multilevel Model 1 Fixed Effects and Standard Error}  
  fixef(m2)
  se.fixef(m2)
```  

```{r Multilevel Model 1 Random Effects}  
  ranef(m2)
```  

```{r Multilevel Model 1 Descriptive Plots}
  plot_model(m2, type = "re")
  plot_model(m2, type = "pred")
  plot_model(m2, type = "slope")
  plot(m2)
```

``` {r Multilevel Model 1 Assumption Tests}
  df$AbsResSqr <- (abs(residuals(m2)))^2
  LeveneTest <- lm(AbsResSqr ~ Emo.Extent.pc.z + Emo.Extent.pmean.z, data=df)
  anova(LeveneTest)
  qqmath(m2, id=0.05)
  rm(LeveneTest)
```  

Our model using person centered affective intensity and covariates as fixed effects, with person-mean affective intensity as a control, performed better than our other models, including a model without fixed effects [ICC = 0.346], a model without covariates. Within this ideal model, person-centered affective intensity [B = 0.312, 95% CI = 0.166, 0.458, p < 0.001] was a significant predictor of regulation effort. Each one standard deviation unit increase in affective intensity of a negative event was met with a 0.312 standard deviation unit increase in the effort to regulate that event. This suggest that our paradigm was sufficiently eliciting regulatory behaviors from participants in a way that logically makes sense. 

```{r Multilevel Model Plot} 
df$fit <- fitted(m2)
plot <- ggplot(data = df, aes(x = Emo.Extent.pc.z, y = fit)) +
                stat_smooth(aes(x = Emo.Extent.pc.z, y = Reg.Extent.z, color = as.factor(PID)), 
                            method = "lm", 
                            alpha = .00, 
                            size = .5) +
                stat_smooth(method="lm", se = T, alpha = .6, size = 1.5, color = "Black") +
                scale_x_continuous(breaks = c(-3,-2,-1,0,1,2,3)) +
                scale_y_continuous(breaks = c(-2,-1,0,1,2)) +
                labs(title = "Emotion Intensity Predicts Regulation Effort",
                     subtitle = "B = 0.312 ***",
                     x = "Emotion Intensity (Person-Centered, z)", 
                     y ="Regulation Effort (z)") +
                coord_cartesian(xlim=c(-2.5, 1.5), ylim=c(-2, 2)) +    
                theme_classic() +
                theme(legend.position = "none") +
                theme(plot.title = element_text(face="bold", size=8, hjust = 0.5)) +
                theme(plot.subtitle = element_text(size = 8, hjust = 0.5, face = "italic")) +
                theme(plot.caption = element_text(size = 4, hjust = 0.00, face = "italic")) +
                theme(axis.title = element_text(size = 10)) +
                theme(axis.text.x = element_text(size = 10, color = "Black")) +
                theme(axis.text.y = element_text(size = 10, color = "Black")) 
plot
```

```{r Exporting Regulation & Intensity Regression}
  setwd(Plots)
  pdf("FN01_EmoExtent_RegExtent.pdf",
      width = 4, 
      height = 3)
  plot
  dev.off()
  rm(plot)
```

# Another Preliminary Analysis

Before I hop into the primary analysis, there's one more thing I wanted to explore. It makes sense that individuals with a high ERQ Reappraisal score should report using reappraisal more frequently than those that do not. I'm going to create a dataframe to explore this

````{r Designating Rows}
rows <- sort(unique(df$PID))
```

````{r Designating Columns}
cols <- c("PID", "Percent.Reapp", "ERQ_Reapp")
```

````{r Designating Columns}
df.pl <- data.frame(matrix(NA, 
                   nrow = length(rows), 
                   ncol = length(cols), 
                   dimnames = list(rows, cols)))
rm(cols)
```

````{r Populating PID, ERQ, and individual difference measures}
df.pl$PID <- rows
for (i in rows){
  df.pl$ERQ_CogReapp[df.pl$PID == i] <- df$ERQ_CogReapp[df$PID == i][1]
}
rm(i)
```

```{r Populating accuracy and percentage of distraction}
df_temp <- subset(df,
                  select = c("PID", "Event.ID", "Reappraisal")) %>%
           distinct()

for (i in rows){
  df.pl$Percent.Reapp[df.pl$PID == i] <- sum(df_temp$Reappraisal[df_temp$PID == i])/length(unique(df_temp$Event.ID))
}

rm(df_temp,i)
```

Ouff, we really don't have enough reappraisal to run this analysis well, so I don't really trust the analysis. 

```{r ERQ Analysis}
  m1 <- lm(Percent.Reapp ~ ERQ_CogReapp, data = df.pl)
  stargazer(m1,type = "text")
  rm(m1)
```

# Primary Analysis

```{r Mixed Effect Logistic Regression Model - NULL}
m0 <- glmer(Attention.Deployment ~ 1 + (1 | PID), data = df, family = binomial)
icc(m0)
```

```{r Mixed Effect Logistic Regression Model - FIXED EFFECT}
m1 <- glmer(Attention.Deployment ~ Emo.Extent.z + (1 | PID), data = df, family = binomial)
m2 <- glmer(Attention.Deployment ~ Emo.Extent.pc.z + Emo.Extent.pmean.z + (1 | PID), data = df, family = binomial)
m3 <- glmer(Attention.Deployment ~ Emo.Extent.pc.z + Emo.Extent.pmean.z + Gender + 
             IUS_Factor2 + STAI_State + STAI_Trait + (1 | PID), data = df, family = binomial)
```

```{r Model Comparisons}  
anova(m0, m1)
anova(m1, m2)
anova(m2, m3)
```


```{r Model Outputs}  
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
class(m3) <- "lmerMod"
stargazer(m0,m1,m2,m3,
          type = "text", 
          title= "Affect Predicting Regulation Usage", 
          column.labels = c("Null Model", "Model 1", "Model 2", "Model 3"),
          dep.var.labels = "Strategy Usage", 
          star.cutoffs = c(0.05, 0.01,0.001),
          covariate.labels = c("Affective Intensity (z)","Affective Intensity (Person-Centered, z)", "Affective Intensity (Person-Mean)", "Gender",
                               "IUS, Factor 2", "STAI, State", "STAI, Trait", "Intercept"), ci = T,
          notes = "")
```

```{r}
exp(fixef(m1))
```

We fail to find a model with predictive value in this relationship.

```{r Mixed Effect Logistic Regression Model - FIXED EFFECT}
m1.e <- effect("Emo.Extent.z",m1,xlevels=list(Emo.Extent.z=seq(range(df$Emo.Extent.z)[1],
                                                       range(df$Emo.Extent.z)[2],
                                                       0.01))) 
m1.e <-as.data.frame(m1.e)
head(m1.e)
plot <- ggplot(m1.e, aes(x=Emo.Extent.z, y=fit)) +
  geom_jitter(data = df, aes(x=Emo.Extent.z, y=Attention.Deployment, color = Attention.Deployment), alpha = 0.5, width = 0.5, height = 0.01) +
  geom_line(color = "Black", size = 1.5) +
  geom_ribbon(aes(ymin=lower, ymax=upper), alpha = 0.15) +
  labs(title = "Emotion Intensity Fails to Predict Strategy Usage") +
  scale_x_continuous("Emotion Intensity (z)", breaks = round(seq(range(df$Emo.Extent.z)[1],
                                                         range(df$Emo.Extent.z)[2], 
                                                         (range(df$Emo.Extent.z)[2] - range(df$Emo.Extent.z)[1])/6),0)) +
  scale_y_continuous("Probability of Using Strategy", breaks = seq(0,1,0.2), labels = c("Reappraisal","0.2","0.4","0.6","0.8", "Distraction")) +
  coord_cartesian(xlim = c(range(df$Emo.Extent.z)[1],
                           range(df$Emo.Extent.z)[2]), ylim = c(0,1)) +
  theme_classic() +  
  theme(legend.position = "none") +
  theme(plot.title = element_text(face="bold", size=8, hjust = 0.5)) +
  theme(plot.subtitle = element_text(size = 8, hjust = 0.5, face = "italic")) +
  theme(plot.caption = element_text(size = 4, hjust = 0.00, face = "italic")) +
  theme(axis.title = element_text(size = 10)) +
  theme(axis.text.x = element_text(size = 10, color = "Black")) +
  theme(axis.text.y = element_text(size = 10, color = "Black")) 
plot
```

```{r Exporting Regulation & Intensity Regression}
  setwd(Plots)
  pdf("FN01_EmoExtent_Strategy.pdf",
      width = 4, 
      height = 3)
  plot
  dev.off()
  rm(plot)
```

# Last Ditch Effort Analysis

One last check: someone could argue that because we captured regulatory behavior at the event level but our dataframe is structured such that each row represents one of the many emotions a person could have experienced for any one event, that we are skewing the results. We can imagine a situation where a person felt 2 fear and 7 tense, but we're attributing the same regulatory strategy to each. Now I'll try to create one last dataframe that sums and averages affective intensity at the event level to see if either demonstrate predictive utility.

```{r Primary Dataframe}
df_temp <- subset(df, select = c("PID", "Event_chronolorder")) %>%
           distinct()
          
df_temp$Emo_Sum <- NA
df_temp$Emo_Mean <- NA
df_temp$Emo_Sum.z <- NA
df_temp$Emo_Mean.z <- NA
df_temp$Emo_Sum_pc.z <- NA
df_temp$Emo_Mean_pc.z <- NA
df_temp$Emo_Sum_pmean.z <- NA
df_temp$Emo_Mean_pmean.z <- NA
df_temp$Attention.Deployment <- NA
df_temp$Gender <- NA
df_temp$IUS_Factor2 <- NA
df_temp$STAI_State <- NA
df_temp$STAI_Trait <- NA

for (i in 1:nrow(df_temp)){
  df_temp$Emo_Sum[i] <- sum(df$Emo.Extent[df$PID == df_temp$PID[i] &
                                          df$Event_chronolorder == df_temp$Event_chronolorder[i]])
  df_temp$Emo_Sum.z[i] <- sum(df$Emo.Extent.z[df$PID == df_temp$PID[i] &
                                          df$Event_chronolorder == df_temp$Event_chronolorder[i]])
  df_temp$Emo_Sum_pc.z[i] <- sum(df$Emo.Extent.pc.z[df$PID == df_temp$PID[i] &
                                        df$Event_chronolorder == df_temp$Event_chronolorder[i]])
  df_temp$Emo_Sum_pmean.z[i] <- sum(df$Emo.Extent.pmean.z[df$PID == df_temp$PID[i] &
                                        df$Event_chronolorder == df_temp$Event_chronolorder[i]])  
  df_temp$Emo_Mean[i] <- mean(df$Emo.Extent[df$PID == df_temp$PID[i] &
                                        df$Event_chronolorder == df_temp$Event_chronolorder[i]])
  df_temp$Emo_Mean.z[i] <- mean(df$Emo.Extent.z[df$PID == df_temp$PID[i] &
                                        df$Event_chronolorder == df_temp$Event_chronolorder[i]])
  df_temp$Emo_Mean_pc.z[i] <- mean(df$Emo.Extent.pc.z[df$PID == df_temp$PID[i] &
                                        df$Event_chronolorder == df_temp$Event_chronolorder[i]])
  df_temp$Emo_Mean_pmean.z[i] <- mean(df$Emo.Extent.pmean.z[df$PID == df_temp$PID[i] &
                                        df$Event_chronolorder == df_temp$Event_chronolorder[i]])
  df_temp$Gender[i] <- df$Gender[df$PID == df_temp$PID[i] &
                                 df$Event_chronolorder == df_temp$Event_chronolorder[i]][1]
  df_temp$IUS_Factor2[i] <- df$IUS_Factor2[df$PID == df_temp$PID[i] &
                                 df$Event_chronolorder == df_temp$Event_chronolorder[i]][1]  
  df_temp$STAI_State[i] <- df$STAI_State[df$PID == df_temp$PID[i] &
                                 df$Event_chronolorder == df_temp$Event_chronolorder[i]][1]
  df_temp$STAI_Trait[i] <- df$STAI_Trait[df$PID == df_temp$PID[i] &
                                 df$Event_chronolorder == df_temp$Event_chronolorder[i]][1] 
  df_temp$Attention.Deployment[i] <- as.numeric(df$Attention.Deployment[df$PID == df_temp$PID[i] &
                                 df$Event_chronolorder == df_temp$Event_chronolorder[i]][1])  
}

write.csv(df_temp, paste0(Export,"df_subset_eventlevel.csv"))
```

```{r Mixed Effect Logistic Regression Model - NULL}
m0 <- glmer(Attention.Deployment ~ 1 + (1 | PID), data = df_temp, family = binomial)
icc(m0)
```

```{r Mixed Effect Logistic Regression Model - FIXED EFFECT}
m1 <- glmer(Attention.Deployment ~ Emo_Sum.z + (1 | PID), data = df_temp, family = binomial)
m2 <- glmer(Attention.Deployment ~ Emo_Sum_pc.z + Emo_Sum_pmean.z + (1 | PID), data = df_temp, family = binomial)
m3 <- glmer(Attention.Deployment ~ Emo_Sum_pc.z + Emo_Sum_pmean.z + Gender + 
             IUS_Factor2 + STAI_State + STAI_Trait + (1 | PID), data = df_temp, family = binomial)
```

```{r Model Comparisons}  
anova(m0, m1)
anova(m1, m2)
anova(m2, m3)
```


```{r Model Outputs}  
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
class(m3) <- "lmerMod"
stargazer(m0,m1,m2,m3,
          type = "text", 
          title= "Affect Predicting Regulation Usage", 
          column.labels = c("Null Model", "Model 1", "Model 2", "Model 3"),
          dep.var.labels = "Strategy Usage", 
          star.cutoffs = c(0.05, 0.01,0.001),
          covariate.labels = c("Affective Intensity (z)", "Affective Intensity (Person-Centered, z)", "Affective Intensity (Person-Mean)", "Gender",
                               "IUS, Factor 2", "STAI, State", "STAI, Trait", "Intercept"),
          notes = "")
```

```{r}
exp(fixef(m1))
# exp(confint(m1))
```

No luck with the sum. Let's try the average. 

```{r Mixed Effect Logistic Regression Model - FIXED EFFECT}
m1.e <- effect("Emo_Sum.z",m1,xlevels=list(Emo_Sum.z=seq(range(df_temp$Emo_Sum.z)[1],
                                                         range(df_temp$Emo_Sum.z)[2],
                                                         0.01))) 
m1.e <-as.data.frame(m1.e)
head(m1.e)
plot <- ggplot(m1.e, aes(x=Emo_Sum.z, y=fit)) +
  geom_jitter(data = df_temp, aes(x=Emo_Sum.z, y=Attention.Deployment, color = Attention.Deployment), alpha = 0.5, width = 0.5, height = 0.01) +
  geom_line(color = "Black", size = 1.5) +
  geom_ribbon(aes(ymin=lower, ymax=upper), alpha = 0.15) +
  labs(title = "Emotion Intensity Fails to Predict Strategy Usage") +
  scale_x_continuous("Emotion Intensity (z)", breaks = round(seq(range(df_temp$Emo_Sum.z)[1],
                                                         range(df_temp$Emo_Sum.z)[2], 
                                                         (range(df_temp$Emo_Sum.z)[2] - range(df_temp$Emo_Sum.z)[1])/6),0)) +
  scale_y_continuous("Probability of Using Strategy", breaks = seq(0,1,0.2), labels = c("Reappraisal","0.2","0.4","0.6","0.8", "Distraction")) +
  coord_cartesian(xlim = c(range(df_temp$Emo_Sum.z)[1],
                           range(df_temp$Emo_Sum.z)[2]), ylim = c(0,1)) +
  theme_classic() +  
  theme(legend.position = "none") +
  theme(plot.title = element_text(face="bold", size=8, hjust = 0.5)) +
  theme(plot.subtitle = element_text(size = 8, hjust = 0.5, face = "italic")) +
  theme(plot.caption = element_text(size = 4, hjust = 0.00, face = "italic")) +
  theme(axis.title = element_text(size = 10)) +
  theme(axis.text.x = element_text(size = 10, color = "Black")) +
  theme(axis.text.y = element_text(size = 10, color = "Black")) 
plot
```

```{r Mixed Effect Logistic Regression Model - FIXED EFFECT}
m1 <- glmer(Attention.Deployment ~ Emo_Mean.z + (1 | PID), data = df_temp, family = binomial)
m2 <- glmer(Attention.Deployment ~ Emo_Mean_pc.z + Emo_Mean_pmean.z + (1 | PID), data = df_temp, family = binomial)
m3 <- glmer(Attention.Deployment ~ Emo_Mean_pc.z + Emo_Mean_pmean.z + Gender + 
             IUS_Factor2 + STAI_State + STAI_Trait + (1 | PID), data = df_temp, family = binomial)
```

```{r Model Comparisons}  
anova(m0, m1)
anova(m1, m2)
anova(m2, m3)
```


```{r Model Outputs}  
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
class(m3) <- "lmerMod"
stargazer(m0,m1,m2,m3,
          type = "text", 
          title= "Affect Predicting Regulation Usage", 
          column.labels = c("Null Model", "Model 1", "Model 2", "Model 3"),
          dep.var.labels = "Strategy Usage", 
          star.cutoffs = c(0.05, 0.01,0.001),
          covariate.labels = c("Affective Intensity (z)", "Affective Intensity (Person-Centered, z)", "Affective Intensity (Person-Mean)", "Gender",
                               "IUS, Factor 2", "STAI, State", "STAI, Trait", "Intercept"),
          notes = "")
```

```{r}
exp(fixef(m1))
```

No luck with the mean either.

```{r Mixed Effect Logistic Regression Model - FIXED EFFECT}
m1.e <- effect("Emo_Mean.z",m1,xlevels=list(Emo_Mean.z=seq(range(df_temp$Emo_Mean.z)[1],
                                                       range(df_temp$Emo_Mean.z)[2],
                                                       0.01))) 
m1.e <-as.data.frame(m1.e)
head(m1.e)
plot <- ggplot(m1.e, aes(x=Emo_Mean.z, y=fit)) +
  geom_jitter(data = df_temp, aes(x=Emo_Mean.z, y=Attention.Deployment, color = Attention.Deployment), alpha = 0.5, width = 0.5, height = 0.01) +
  geom_line(color = "Black", size = 1.5) +
  geom_ribbon(aes(ymin=lower, ymax=upper), alpha = 0.15) +
  labs(title = "Emotion Intensity Fails to Predict Strategy Usage") +
  scale_x_continuous("Emotion Intensity (z)", breaks = round(seq(range(df_temp$Emo_Mean.z)[1],
                                                         range(df_temp$Emo_Mean.z)[2], 
                                                         (range(df_temp$Emo_Mean.z)[2] - range(df_temp$Emo_Mean.z)[1])/6),0)) +
  scale_y_continuous("Probability of Using Strategy", breaks = seq(0,1,0.2), labels = c("Reappraisal","0.2","0.4","0.6","0.8", "Distraction")) +
  coord_cartesian(xlim = c(range(df_temp$Emo_Mean.z)[1],
                           range(df_temp$Emo_Mean.z)[2]), ylim = c(0,1)) +
  theme_classic() +  
  theme(legend.position = "none") +
  theme(plot.title = element_text(face="bold", size=8, hjust = 0.5)) +
  theme(plot.subtitle = element_text(size = 8, hjust = 0.5, face = "italic")) +
  theme(plot.caption = element_text(size = 4, hjust = 0.00, face = "italic")) +
  theme(axis.title = element_text(size = 10)) +
  theme(axis.text.x = element_text(size = 10, color = "Black")) +
  theme(axis.text.y = element_text(size = 10, color = "Black")) 
plot
```

I think I've exhausted all my options and have come to the conclusion that we cannot establish a relationship between affective intensity and affective usage within this specific dataset. Importantly, we are exploring this in circumstances where affective intensity is relatively high, the stimuli are dynamic, multimodal, and proximal (rather than decontextualized and/or distal). 

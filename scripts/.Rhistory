m6 <- glmer(Strategy.FU ~ Emo.Extent.z + Age + Gender.Identity + IUS_F2.FU + DERS.LimitAccess.FU + (1 | PID.FU) + (1 | PID.OG), data = df.fu, family = binomial)
anova(m0, m1)
anova(m1, m2)
anova(m1, m3)
anova(m1, m4)
anova(m1, m5)
anova(m1, m6)
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
class(m3) <- "lmerMod"
class(m4) <- "lmerMod"
class(m5) <- "lmerMod"
class(m6) <- "lmerMod"
stargazer(m0,m1,m2, m3,m4,m5, m6,
type = "text",
title= "Intensity Predicting Regulation",
column.labels = c("Null Model", "Model 1", "Model 2", "Model 3", "Model 4", "Model 5", "Model 6"),
dep.var.labels = "Strategy Selection",
star.cutoffs = c(0.05, 0.01,0.001),
covariate.labels = c("Affective Intensity (z)", "Age",
"Gender, Male",
"Gender, Non-Binary",
"IUS, Factor 2",
"DERS Limited Access to Strategies",
"Intercept"), ci = T,
notes = "")
exp(fixef(m1))
exp(0.017)
exp(0.091)
m1.e <- effect("Emo.Extent.z",m1,xlevels=list(Emo.Extent.z=seq(range(df.fu$Emo.Extent.z)[1],
range(df.fu$Emo.Extent.z)[2],
0.01)))
m1.e <-as.data.frame(m1.e)
head(m1.e)
plot <- ggplot(m1.e, aes(x=Emo.Extent.z, y=fit)) +
geom_jitter(data = df.fu, aes(x=Emo.Extent.z, y=Strategy.FU_num,
color=as.factor(Strategy.FU_num)),
width = 0.25, height = 0.035, size = 1, alpha = 0.10) +
geom_line(color = "Black", size = 4.5) +
geom_ribbon(aes(ymin=lower, ymax=upper), alpha = 0.15) +
scale_x_continuous("Emotion Intensity (z)", breaks = round(seq(range(df.fu$Emo.Extent.z)[1],
range(df.fu$Emo.Extent.z)[2],
(range(df.fu$Emo.Extent.z)[2] - range(df.fu$Emo.Extent.z)[1])/6),0)) +
scale_y_continuous("Probability of Choosing Distraction", breaks = seq(0,1,0.20)) +
scale_color_manual(values = c("#F8766D", "#00BFC4")) +
coord_cartesian(xlim = c(range(df.fu$Emo.Extent.z)[1],
range(df.fu$Emo.Extent.z)[2]), ylim = c(0,1)) +
theme_classic() +
theme(legend.position = "none") +
theme(axis.title = element_text(size = 32, color = "Black")) +
theme(axis.text.x = element_text(size = 36, color = "Black")) +
theme(axis.text.y = element_text(size = 30, color = "Black"))
plot
tiff(paste0(Plots, "Fig8.tiff"),
res = 300,
units = "in",
width = 6,
height = 4.5)
plot
dev.off()
rm(plot, m0, m1, m2, m3, m1.e)
m0 <- glmer(Strategy.FU ~ 1 + (1 | PID.FU) + (1 | PID.OG), data = df.fu, family = binomial)
m1 <- glmer(Strategy.FU ~ Emo.Extent.z + (1 | PID.FU) + (1 | PID.OG), data = df.fu, family = binomial)
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
exp(fixef(m1))
exp(0.017)
exp(0.091)
m1.e <- effect("Emo.Extent.z",m1,xlevels=list(Emo.Extent.z=seq(range(df.fu$Emo.Extent.z)[1],
range(df.fu$Emo.Extent.z)[2],
0.01)))
m1.e <-as.data.frame(m1.e)
head(m1.e)
plot <- ggplot(m1.e, aes(x=Emo.Extent.z, y=fit)) +
geom_jitter(data = df.fu, aes(x=Emo.Extent.z, y=Strategy.FU_num,
color=as.factor(Strategy.FU_num)),
width = 0.25, height = 0.035, size = 1, alpha = 0.10) +
geom_line(color = "Black", size = 4.5) +
geom_ribbon(aes(ymin=lower, ymax=upper), alpha = 0.15) +
scale_x_continuous("Emotion Intensity (z)", breaks = round(seq(range(df.fu$Emo.Extent.z)[1],
range(df.fu$Emo.Extent.z)[2],
(range(df.fu$Emo.Extent.z)[2] - range(df.fu$Emo.Extent.z)[1])/6),0)) +
scale_y_continuous("Probability of Choosing Distraction", breaks = seq(0,1,0.20)) +
scale_color_manual(values = c("#F8766D", "#00BFC4")) +
coord_cartesian(xlim = c(range(df.fu$Emo.Extent.z)[1],
range(df.fu$Emo.Extent.z)[2]), ylim = c(0,1)) +
theme_classic() +
theme(legend.position = "none") +
theme(axis.title = element_text(size = 32, color = "Black")) +
theme(axis.text.x = element_text(size = 36, color = "Black")) +
theme(axis.text.y = element_text(size = 30, color = "Black"))
plot
tiff(paste0(Plots, "Fig8.tiff"),
res = 300,
units = "in",
width = 9,
height = 7.75)
plot
plot
dev.off()
tiff(paste0(Plots, "Fig8.tiff"),
res = 300,
units = "in",
width = 9,
height = 7.75)
plot
dev.off()
tiff(paste0(Plots, "Fig8.tiff"),
res = 300,
units = "in",
width = 12,
height = 9)
plot
dev.off()
# If the pacman package manager is not currently installed on this system, install it.
if (require("pacman") == FALSE){
install.packages("pacman")
}
# Loading in my packages with my pacman manager
pacman::p_load(effects,
here,
influence.ME,
lattice,
lme4,
lmerTest,
performance,
sjPlot,
stargazer,
tidyverse)
# Loading a custom function that will clean NA values from dataframes
source("https://github.com/wj-mitchell/stinkR/blob/main/remove_NAs.R?raw=TRUE", local = T)
# Loading a custom function that will just make building blanke dataframes simpler
source("https://github.com/wj-mitchell/stinkR/blob/main/make_df.R?raw=TRUE", local = T)
options(scipen=100)
options(digits=3)
options(tinytex.verbose = TRUE)
# Identifying our main directory
WorkDir <- here::here()
# Identifying specific directories to read and write from
Import <- paste0(WorkDir, "/data/Study_01/Raw/")
Plots <- paste0(WorkDir, "/plots/")
df <- read.csv(file = paste0(Import, "df.csv"),
header = T,
sep=",",
row.names = 1,
stringsAsFactors = F,
na.strings=c("","NA", "N/A"))
df_all <- df %>%
subset(!is.na(df$EmoMod) &
df$EmoMod != "" &
!is.na(df$PID))
df_all$Disengage <- NA
df_all$Disengage_cat <- NA
df_all$Disengage[(df_all$Attention.Deployment == 1 | df_all$Response.Modulation == 1) & df_all$Reappraisal == 0] <- 1
df_all$Disengage_cat[(df_all$Attention.Deployment == 1 | df_all$Response.Modulation == 1) & df_all$Reappraisal == 0] <- "Disengage"
df_all$Disengage[df_all$Attention.Deployment == 0 & df_all$Response.Modulation == 0 & df_all$Reappraisal == 1] <- 0
df_all$Disengage_cat[df_all$Attention.Deployment == 0 & df_all$Response.Modulation == 0 & df_all$Reappraisal == 1] <- "Engage"
df_all$Distracted <- NA
df_all$Distracted_cat <- NA
df_all$Distracted[df_all$Attention.Deployment == 1 & df_all$Response.Modulation == 0 & df_all$Reappraisal == 0] <- 1
df_all$Distracted_cat[df_all$Attention.Deployment == 1 & df_all$Response.Modulation == 0 & df_all$Reappraisal == 0] <- "Distracted"
df_all$Distracted[df_all$Attention.Deployment == 0 & df_all$Response.Modulation == 0 & df_all$Reappraisal == 1] <- 0
df_all$Distracted_cat[df_all$Attention.Deployment == 0 & df_all$Response.Modulation == 0 & df_all$Reappraisal == 1] <- "Reappraised"
df_all$Intense.z <- as.numeric(df_all$Intense.z)
df_all$CogLoad_Post <- as.numeric(df_all$CogLoad_Post)
df_all$Intense.pmean.z <- as.numeric(df_all$Intense.pmean.z)
df_sub <- subset(df_all,
df_all$Reg == "Decrease" &
df_all$Timing != "MemReg")
df_pl <- data.frame("PID" = unique(df_sub$PID),
"PropDist" = NA)
df_pl <- subset(df_sub,
!is.na(df_sub$HH.Enjoy) ,
select = c("PID", names(df_sub)[grep(names(df_sub), pattern = "Motive")],
"Pos.Anticipate", "Neg.Anticipate", "CogLoad_Post", "CogLoad_Pre",
"Cond", "Date_HH", "Time_HH", "Group", "Fear.Before", "Fear.During",
"Intense.pmean.z", "Fear.Enjoy", "HH.Enjoy", "Startle", "Peer.Presence",
"Sex", "Age", names(df_sub)[which(names(df_sub) == "BDI_Total"):which(names(df_sub) == "IRQ_PE")])) %>%
distinct() %>%
merge(x = df_pl,
y = .,
by = "PID",
all.x = F)
for (i in 1:nrow(df_pl)){
total <- nrow(subset(df_sub,
df_sub$PID == df_pl$PID[i]))
dist <- nrow(subset(df_sub,
df_sub$PID == df_pl$PID[i] & df_sub$Distracted == 1))
if (total > 0) {
df_pl$PropDist[i] <- dist/total
}
}
rm(i, total, dist)
(model <- t.test(df_sub$Intense.z[df_sub$Timing == "ImmReg"],
df_sub$Intense.z[df_sub$Timing == "DelReg"],
alternative = "two.sided",
paired = F))
results <- report::report(model)
df_sub$Timing <- factor(df_sub$Timing, levels = c("ImmReg", "DelReg"))
ggplot(data = df_sub, aes(x = Timing, y = Intense.z)) +
theme_classic()+
geom_jitter(aes(alpha=0.5, color=Timing),shape=16, position=position_jitter(0.2)) +
geom_violin(trim=T, alpha=0.25) +
labs(title = "Time Influenced Emotional Intensity",
subtitle = paste("No differences were observed between novel and familiar recalled events."),
x = NULL,
y ="Emotion Intensity") +
scale_y_continuous(breaks = c(-2:2)) +
coord_cartesian(ylim=c(-2.25,2.25)) +
theme(legend.position="none") +
geom_boxplot(width=0.2, color="black", alpha=0.0) +
theme(plot.title = element_text(face="bold", size=8, hjust = 0.5)) +
theme(plot.subtitle = element_text(size = 6, hjust = 0.5, face = "italic")) +
theme(plot.caption = element_text(size = 8, hjust = 0.0, face = "italic")) +
theme(axis.title = element_text(size = 14)) +
theme(axis.text.x = element_text(size = 10, color = "Black", angle=-45, vjust = 1, hjust = 0.0)) +
theme(axis.text.y = element_text(size = 12, color = "Black"))
(model <- chisq.test(x = df_sub$Timing, y= df_sub$Distracted_cat))
ggplot(data = subset(df_sub, !is.na(df_sub$Distracted_cat)), aes(x = Timing, color = Distracted_cat, fill = Distracted_cat)) +
geom_bar() +
scale_x_discrete("Timepoint", breaks = c("Immediate Recall", "Novel Recall")) +
scale_y_continuous(breaks = c(0,100,200)) +
labs(title = "Frequency of Strategy Usage by Time Reported",
subtitle = "Strategies were reported equally as frequently across timepoints",
x = NULL,
y ="Frequency") +
scale_color_brewer(palette = "Dark2") +
scale_fill_brewer(palette = "Set2") +
coord_cartesian(ylim=c(0.0, 200.0)) +
theme_classic() +
theme(plot.title = element_text(face="bold", size=8, hjust = 0.5)) +
theme(plot.subtitle = element_text(size = 10, hjust = 0.5, face = "italic")) +
theme(plot.caption = element_text(size = 8, hjust = 0.0, face = "italic")) +
theme(axis.title = element_text(size = 10)) +
theme(axis.text.x = element_text(size = 14, color = "Black")) +
theme(axis.text.y = element_text(size = 12, color = "Black")) +
theme(legend.key.size = unit(0.5, 'cm')) +
theme(legend.title = element_text(size=8)) +
theme(legend.text = element_text(size=6))
results <- c(results, "A Person's chi-squared test suggests that no differences exist in the endorsement of strategy usage by timepoint (x2 = 0.001, df = 1, p = 1)")
summary(lm(PropDist ~ CogLoad_Post, data = df_pl))
summary(lm(Intense.pmean.z ~ CogLoad_Post, data = df_pl))
summary(lm(PropDist ~ Intense.pmean.z + CogLoad_Post, data = df_pl))
summary(lm(PropDist ~ CogLoad_Pre, data = df_pl))
summary(lm(Intense.pmean.z ~ CogLoad_Pre, data = df_pl))
summary(lm(PropDist ~ Intense.pmean.z + CogLoad_Pre, data = df_pl))
results <- c(results, "When examining the association between cognitive load as measured immediately after the haunted house relative to each participant's baseline cognitive load and affective intensity, we fail to find any significant association (b = 0.00, se = 0.05, t(74) = 0.04, p = 0.97). When examining cognitive load's predictive utility towards strategy usage, we also fail to find any significant association (b = -0.01, se = 0.01, t(74) = -1.26, p = 0.21), even when adjusting for affective intensity within the model (b = -0.02, se = 0.01, t(73) = -1.27, p = 0.21).")
summary(lm(PropDist ~ Pos.Anticipate + Neg.Anticipate + Fear.Before, data = df_pl))
summary(lm(Intense.pmean.z ~ Pos.Anticipate + Neg.Anticipate + Fear.Before, data = df_pl))
results <- c(results,
report::report(lm(PropDist ~ Pos.Anticipate + Neg.Anticipate + Fear.Before, data = df_pl)),
report::report(lm(Intense.pmean.z ~ Pos.Anticipate + Neg.Anticipate + Fear.Before, data = df_pl)))
for (i in grep(names(df_pl), pattern = "Motive")){
df_pl[,i] <- as.numeric(df_pl[,i])
}
model1 <- lm(PropDist ~ Motive.Payment + Motive.Thrill + Motive.Novelty + Motive.Challenge + Motive.Peers + Motive.Science + Motive.Bored, data = df_pl)
summary(model1)
model2 <- lm(Intense.pmean.z ~ Motive.Payment + Motive.Thrill + Motive.Novelty + Motive.Challenge + Motive.Peers + Motive.Science + Motive.Bored, data = df_pl)
summary(model2)
results <- c(results,
report::report(model1),
report::report(model2))
(model <- cor.test(df_all$HH.Enjoy,
df_all$Fear.Enjoy,
use = "everything"))
results <- c(results,
report::report(model))
summary(lm(PropDist ~ HH.Enjoy + Fear.Enjoy + Fear.During, data = df_pl))
summary(lm(Intense.pmean.z ~ HH.Enjoy + Fear.Enjoy + Fear.During, data = df_pl))
model1 <- lm(PropDist ~ HH.Enjoy + Fear.Enjoy * Fear.During, data = df_pl)
summary(model1)
model2 <- lm(Intense.pmean.z ~ HH.Enjoy + Fear.Enjoy * Fear.During, data = df_pl)
summary(model2)
results <- c(results,
report::report(model1),
report::report(model2))
model1 <- lm(PropDist ~ Age + Sex, data = df_pl)
summary(model1)
model2 <- lm(Intense.pmean.z ~ Age + Sex, data = df_pl)
summary(model2)
results <- c(results,
report::report(model1),
report::report(model2))
(model1 <- t.test(df_pl$PropDist[df_pl$Peer.Presence == "Yes"],
df_pl$PropDist[df_pl$Peer.Presence == "No"]))
(model2 <- t.test(df_pl$Intense.pmean.z[df_pl$Peer.Presence == "Yes"],
df_pl$Intense.pmean.z[df_pl$Peer.Presence == "No"]))
results <- c(results,
report::report(model1),
report::report(model2))
cols <- c(which(names(df_pl) == "BDI_Total"):which(names(df_pl) == "IRQ_PE"))
# Iterating through each of our potential covariates
for (VAR in 1:(length(cols))){
# Pulling up our outcome variable
item <- df_pl[,which(names(df_pl) == "Intense.pmean.z")] %>%
# Converting the data to numeric from double
unlist() %>%
# Running a correlation test between the covariate and outcome
cor.test(x = .,
y = df_pl[,cols[VAR]],
use = "pairwise.complete.obs")
# If the given correlation is significant
if (item$p.value < 0.05){
# Report the statistics
print(paste0(names(df_pl)[cols[VAR]],
" demonstrates a significant association to our outcome variable (t = ",
round(item$statistic, 3), ", r = ", round(item$estimate, 3), ", p = ", round(item$p.value, 3),")"))
}
# If the given correlation is non-significant
if (item$p.value >= 0.05){
#Report the statistics
print(paste0(names(df_pl)[cols[VAR]],
": NO SIGNIFICANT ASSOCIATION w/ OUTCOME (t = ",
round(item$statistic,3), ", r = ", round(item$estimate,3), ", p = ", round(item$p.value,3),")"))
}
}
# Clean our space
rm(item, VAR)
# Iterating through each of our potential covariates
for (VAR in 1:(length(cols))){
# Pulling up our outcome variable
item <- df_pl[,which(names(df_pl) == "PropDist")] %>%
# Converting the data to numeric from double
unlist() %>%
# Running a correlation test between the covariate and outcome
cor.test(x = .,
y = df_pl[,cols[VAR]],
use = "pairwise.complete.obs")
# If the given correlation is significant
if (item$p.value < 0.05){
# Report the statistics
print(paste0(names(df_pl)[cols[VAR]],
" demonstrates a significant association to our outcome variable (t = ",
round(item$statistic, 3), ", r = ", round(item$estimate, 3), ", p = ", round(item$p.value, 3),")"))
}
# If the given correlation is non-significant
if (item$p.value >= 0.05){
#Report the statistics
print(paste0(names(df_pl)[cols[VAR]],
": NO SIGNIFICANT ASSOCIATION w/ OUTCOME (t = ",
round(item$statistic,3), ", r = ", round(item$estimate,3), ", p = ", round(item$p.value,3),")"))
}
}
# Clean our space
rm(item, VAR, cols)
model1 <- aov(PropDist ~ Time_HH, data=df_pl)
summary(model1)
model2 <- aov(Intense.pmean.z~ Time_HH, data=df_pl)
summary(model2)
results <- c(results,
report::report(model1),
report::report(model2))
model1 <- aov(PropDist ~ Date_HH, data=df_pl)
summary(model1)
model2 <- aov(Intense.pmean.z~ Date_HH, data=df_pl)
summary(model2)
results <- c(results,
report::report(model1),
report::report(model2))
model1 <- aov(PropDist ~ Group, data=df_pl)
summary(model1)
model2 <- aov(Intense.pmean.z~ Group, data=df_pl)
summary(model2)
results <- c(results,
report::report(model1),
report::report(model2))
summary(lm(PropDist ~ CogLoad_Post, data = df_pl))
summary(lm(Intense.pmean.z ~ CogLoad_Post, data = df_pl))
summary(lm(PropDist ~ Intense.pmean.z + CogLoad_Post, data = df_pl))
summary(lm(PropDist ~ Intense.pmean.z * CogLoad_Post, data = df_pl))
summary(lm(PropDist ~ CogLoad_Pre, data = df_pl))
summary(lm(Intense.pmean.z ~ CogLoad_Pre, data = df_pl))
summary(lm(PropDist ~ Intense.pmean.z + CogLoad_Pre, data = df_pl))
summary(lm(PropDist ~ Intense.pmean.z * CogLoad_Pre, data = df_pl))
if (require("pacman") == FALSE){
install.packages("pacman")
}
pacman::p_load(here, lme4, lmerTest, DHARMa, tidyverse)
here()
source("func_pavlovia_cleaner.R")
files <- list.files("../data/",
full.names = T) %>%
.[file.size(.) > 20000]
## Iterating Through Each File in the Source}
for (FILE in 1:length(files)){
## If this is the first file we're reading ...
if (FILE == 1){
## ... Define it as df
df <- pavlovia_cleaner(files[FILE])
}
## If this is a later file we're reading ...
if (FILE > 1){
## ... Process it ...
df_ <- pavlovia_cleaner(files[FILE])
## ... and then append it to df.
df <- bind_rows(df,
df_)
}
## If this is the last iteration ...
if (FILE == length(files)){
## Clean Our Space
rm(pavlovia_cleaner, FILE, files, df_)
}
}
## If people failed the attention check, get rid of them
df <- df[df$Attention_Check == 4,]
## If people are familiar with the stimuli, get rid of that stimulus
for (PID in unique(df$PID)){
if (any(!is.na(df$Familiar_specific[df$PID == PID]))){
array <- df$Familiar_specific[df$PID == PID & !is.na(df$PID)][1] %>%
strsplit(" ") %>%
as.data.frame() %>%
.[,1]
for (VID in 1:length(array)){
df <- df[-which(df$PID == PID & !is.na(df$PID) & df$Stimulus == array[VID]),]
}
}
}
df$StimInt <- NA
df$StimInt[df$Stimulus == "ARF1665" | df$Stimulus == "HIU8424"] <- "High"
df$StimInt[df$Stimulus == "CNL1892" | df$Stimulus == "FGV3524"] <- "Low"
df$Regulated <- NA
df$Regulated[df$Choice == "Distraction" | df$Choice == "Reappraisal"] <- 1
df$Regulated[df$Choice == "Neither"] <- 0
df$Distracted <- NA
df$Distracted[df$Choice == "Distraction"] <- 1
df$Distracted[df$Choice == "Reappraisal"] <- 0
df$PID[is.na(df$PID)] <- "Missing"
df$IntAfter <- as.numeric(df$IntAfter)
df$IntBefore <- as.numeric(df$IntBefore)
df$IntReduce <- as.numeric(df$IntReduce)
df$Condition <- as.factor(df$Condition)
df$Date <- as.factor(df$Date)
df$IntBefore_z <- as.numeric(scale(df$IntBefore))
df$IntAfter_z <- as.numeric(scale(df$IntAfter))
df$IntReduce_z <- as.numeric(scale(df$IntReduce))
(t_test_result <- t.test(x = df$IntAfter[df$StimInt == "High"],
y = df$IntAfter[df$StimInt == "Low"]))
results <- data.frame(
Group = c("High", "Low"),
rating = t_test_result$estimate,
t_value = t_test_result$statistic,
p_value = t_test_result$p.value
)
# Define the plot
ggplot(results, aes(x = Group, y = rating, fill = Group)) +
geom_bar(stat = "identity", position = "dodge", width = 0.5) +
geom_text(aes(label = sprintf("Rating = %.2f", rating), y = rating), vjust = -0.5, size = 4) +
labs(title = "Rating versus Category", y = "Rating") +
scale_y_continuous(limits = c(0,100), breaks = seq(0,100,20)) +
theme_minimal()
(t_test_result <- t.test(x = df$IntAfter[df$Regulated == 1],
y = df$IntAfter[df$Regulated == 0]))
results <- data.frame(
Group = c("Regulated", "Unregulated"),
rating = t_test_result$estimate,
t_value = t_test_result$statistic,
p_value = t_test_result$p.value
)
# Define the plot
ggplot(results, aes(x = Group, y = rating, fill = Group)) +
geom_bar(stat = "identity", position = "dodge", width = 0.5) +
geom_text(aes(label = sprintf("Rating = %.2f", rating), y = rating), vjust = -0.5, size = 4) +
labs(title = "Rating versus Regulation", y = "Rating") +
scale_y_continuous(limits = c(0,100), breaks = seq(0,100,20)) +
theme_minimal()
hist(df$IntBefore, breaks = seq(0,100,10))
hist(df$IntAfter, breaks = seq(0,100,10))
m1 <- glmer(Regulated ~ IntAfter * Condition + (1|PID),
data = df,
family = "binomial")
summary(m1)
sjPlot::plot_model(m1, type = "int")
m2 <- glmer(Distracted ~ IntAfter * Condition + (1 | PID),
data = df,
family = "binomial")
summary(m2)
sjPlot::plot_model(m2, type = "int")
skimr::skim(df$IntBefore)
probabilities <- predict(m2,
type = "response")
# Bind the logit and tidying the data for plot
df_assum <- df %>%
subset(!is.na(.$Distracted),
select = c("Distracted", "IntAfter")) %>%
mutate(logit = log(probabilities/(1 - probabilities))) %>%
gather(key = "predictors", value = "predictor.value", -logit)
# Examining linearity
ggplot(df_assum, aes(logit, predictor.value))+
geom_point(size = 0.5, alpha = 0.5) +
geom_smooth(method = "loess") +
theme_bw() +
facet_wrap(~predictors, scales = "free_y")
# Calculate Cook's distance
cooksd_values <- cooks.distance(m2)
# Plotting Cook's Distances
threshold <- 0.5
plot(cooksd_values, type = "p", pch = 19, main = "Cook's Distance Plot", xlab = "Observation Index", ylab = "Cook's Distance")
text(which(cooksd_values > threshold), cooksd_values[cooksd_values > threshold], labels = which(cooksd_values > threshold), pos = 3)
# Calculating simulated standardized residuals
residuals_sim <- simulateResiduals(m2, n = 1000)
# Plotting standardized residuals
plot(residuals_sim)
test_res <- testResiduals(residuals_sim)
summary(test_res)
performance::check_model(m2)
t.test(x = df$Effort[df$Choice == "Distraction"],
y = df$Effort[df$Choice == "Reappraisal"])
m3 <- lm(Success ~ Choice * IntAfter, data = df)
summary(m3)

---
title: "ER Choice v. Usage - Study_2"
author: "BLINDED"
date: "`r Sys.Date()`"
output: html_document
---

NAVIGATION / QUESTIONS:
+ Study 1 found that the strategies individuals described using apporximately one week later were not related to the affective intensity of the experiences they underwent. However there were a number of limitations inherent to this study. For one, it's unclear how reliable the memory of emotion regulation strategy usage is one week after usage. It also might be the case that emotion regulation is not applied at the level of the event, which might contain multiple emotions, but at the level of the emotion. We also were left to infer the directionality of regulation based upon descriptions. Lastly, there are a number of covariates which might be important to capture 

PREDICTIONS:
+ We do not believe that negative affective intensity will continue to demonstrate predictive utility towards ER usage (i.e., distraction v. reappraisal) when measured beyond a lab context. We will adjust for a number of individual difference measures in the process.

ISSUES:
+ We captured a number of covariates relevant to regulation strategy choice such as cognitive load and affective expectations. However, at the analysis stage, we found that we lacked the volume of data necessary to incorporate these variables into our models. As such we ran supplemental analyses, to measure their influence upon regulation strategy usage directly and found no association between cogntive load and strategy usage, 

NOTES:

# Setup

```{r Packages I Use}
  # If the pacman package manager is not currently installed on this system, install it.
  if (require("pacman") == FALSE){
    install.packages("pacman")
  }

  # Loading in my packages with my pacman manager
  pacman::p_load(effects,
                 here,
                 influence.ME,
                 lattice,
                 lme4, 
                 performance,
                 sjPlot,
                 stargazer, 
                 tidyverse)
    
  # Loading a custom function that will clean NA values from dataframes
  source("https://github.com/Wjpmitchell3/stinkR/blob/main/remove_NAs.R?raw=TRUE", local = T)
  # Loading a custom function that will just make building blanke dataframes simpler
  source("https://github.com/Wjpmitchell3/stinkR/blob/main/make_df.R?raw=TRUE", local = T)
```

``` {r Number Formatting}
  options(scipen=100)
  options(digits=3)
  options(tinytex.verbose = TRUE)
```

The `here()` function should ideally identify our upper-level project directory. So, `WorkDir` below should be a filepath that looks something like `C:/Users/.../Documents/GitHub/fright_night_study/", or where ever you store these files. Then we'll have additional variables store filepaths within that directory for where we're reading the data from and where we're sending data to. 

```{r Setting Working Directories}
  # Identifying our main directory
  WorkDir <- here::here()

  # Identifying specific directories to read and write from
  Import <- paste0(WorkDir, "/data/Study_02/Raw/")
  Plots <- paste0(WorkDir, "/plots/")
```

# Loading in the data

```{r Pulling in Dataframe}
  df <- read.csv(file = paste0(Import, "df.csv"), 
                 header = T, 
                 sep=",",
                 row.names = 1, 
                 stringsAsFactors = F,
                 na.strings=c("","NA", "N/A"))
  df_all <- df
  df <- df %>%
        subset(!is.na(df$EmoMod) &
               df$EmoMod != "" & 
               !is.na(df$PID))
```

Here we are again creating a binary variable to track whether any given observation was regulated with either strategy, but not both, and we are creating both a quantiative and qualitative variable.

```{r Creating a Regulatory Superfactor}
df$Distracted <- NA
df$Distracted_cat <- NA
df$Distracted[df$Attention.Deployment == 1 & df$Response.Modulation == 0 & df$Reappraisal == 0] <- 1
df$Distracted_cat[df$Attention.Deployment == 1 & df$Response.Modulation == 0 & df$Reappraisal == 0] <- "Distracted"
df$Distracted[df$Attention.Deployment == 0 & df$Response.Modulation == 0 & df$Reappraisal == 1] <- 0
df$Distracted_cat[df$Attention.Deployment == 0 & df$Response.Modulation == 0 & df$Reappraisal == 1] <- "Reappraised"
```

Unfortunately, in exporting and importing the .csv, some of the data is not in the proper structure anymore, so we need to fix that.

```{r Coercing data into the appropriate structure}
df$Intense.z <- as.numeric(df$Intense.z)
df$CogLoad_Post <- as.numeric(df$CogLoad_Post)
df$Intense.pmean.z <- as.numeric(df$Intense.pmean.z)
```

Now, we are going to subset a dataset that mirrors lab studies, so specifically observations wherein one of the two strategies were used, the valence is negative, and the participant was attempting to downregulated according to self-report. 

```{r Creating a Subset of Data}
df <- subset(df, df$Reg == "Decrease" & !is.na(df$Distracted) & df$Valence <= 0.5)
```

# Exploring the Data

This has once again only left us with a fraction of the original dataset (12.3% )

```{r Printing Details}
paste("Number of Participants:", length(unique(df$PID)))
paste("Number of Events:", nrow(distinct(subset(df, select = c("PID", "Timing", "EventNum")))))
paste("Number of Immediate Events:", nrow(distinct(subset(df, df$Timing == "ImmReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Familiar Recall Events:", nrow(distinct(subset(df, df$Timing == "MemReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Novel Recall Events:", nrow(distinct(subset(df, df$Timing == "DelReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Distracted cases:", length(which(df$Distracted == 1)))
paste("Number of Reappraised cases:", length(which(df$Distracted == 0)))
```

## Timing's Influence on Intensity

First, related to previous limitations, I want to examine how affective intensity changes as a result of time. 

```{r Differences in Affective Intensity By Time}
aov(Intense.z ~ Timing, data = df) %>%
  summary()
```

We have a significant result from the ANOVA (F(2,433) = 3.62, p < 0.05). Let's run some contrasts

```{r Immediate versus Familiar Contrast}
t.test(df$Intense.z[df$Timing == "ImmReg"],
       df$Intense.z[df$Timing == "MemReg"],
       alternative = "two.sided",
       paired = F)
```

This result suggests there is an overall trend of a  significant deprecation in recalled affective intensity over time. We'd have to track trajectories on a person by person basis in order to know for sure. I wonder if over time individuals are also more likely to endorse reappraisal versus distraction

```{r Immediate versus Novel Contrast}
t.test(df$Intense.z[df$Timing == "ImmReg"],
       df$Intense.z[df$Timing == "DelReg"],
       alternative = "two.sided",
       paired = F)
```

Yet interestingly the difference between immediate novel and delayed novel only trends significant . . . 

```{r Familiar versus Novel Contrast}
t.test(df$Intense.z[df$Timing == "MemReg"],
       df$Intense.z[df$Timing == "DelReg"],
       alternative = "two.sided",
       paired = F)
```

And the same exact event recalled later is not statistically different than an event recalled later. In study 1, we found that individuals recalled events in a descending order of affective intensity, which could potentially explain this result, in combination with Joanne's finding. Over time recalled events depreciate in intensity and at the same time, the events recalled later were recalled later because they were of a lower affective intensity.  

```{r Visualizing Differences in Affective Intensity Over Time}
df$Timing <- factor(df$Timing, levels = c("ImmReg", "MemReg", "DelReg"))
ggplot(data = df, aes(x = Timing, y = Intense.z)) + 
       theme_classic()+
       geom_jitter(aes(alpha=0.5, color=Timing),shape=16, position=position_jitter(0.2)) + 
       geom_violin(trim=T, alpha=0.25) +
      labs(title = "Time Influenced Emotional Intensity",
                  subtitle = paste("Events reported immediately after exposure were more intense than novel events recalled one week later, and the same events reacalled one week later.\n No differences were observed between novel and familiar recalled events."),
                   x = NULL,
                   y ="Emotion Intensity") +
       scale_y_continuous(breaks = c(-2:2)) +
       coord_cartesian(ylim=c(-2.25,2.25)) +
       theme(legend.position="none") +
       geom_boxplot(width=0.2, color="black", alpha=0.0) +
       theme(plot.title = element_text(face="bold", size=8, hjust = 0.5)) +
       theme(plot.subtitle = element_text(size = 6, hjust = 0.5, face = "italic")) +
       theme(plot.caption = element_text(size = 8, hjust = 0.0, face = "italic")) +
       theme(axis.title = element_text(size = 14)) +
       theme(axis.text.x = element_text(size = 10, color = "Black", angle=-45, vjust = 1, hjust = 0.0)) +
       theme(axis.text.y = element_text(size = 12, color = "Black")) 
```

## Timing's Influence on Strategy Usage

I'm curious whether any differences exist in how frequently strategies were reported by timepoint.

```{r Frequency of Strategies at different time points}
  chisq.test(x = df$Timing, y= df$Distracted_cat)
  ggplot(data = df, aes(x = Timing, color = Distracted_cat, fill = Distracted_cat)) +
        geom_bar() +
        scale_x_discrete("Timepoint", breaks = c("Immediate Recall", "Familiar Recall", "Novel Recall")) +
        scale_y_continuous(breaks = c(0,100,200)) +
        labs(title = "Frequency of Strategy Usage by Time Reported",
            subtitle = "Strategies were reported equally as frequently across timepoints",
             x = NULL,
             y ="Frequency") +
        scale_color_brewer(palette = "Dark2") +
        scale_fill_brewer(palette = "Set2") +
        coord_cartesian(ylim=c(0.0, 200.0)) +
        theme_classic() +
        theme(plot.title = element_text(face="bold", size=8, hjust = 0.5)) +
        theme(plot.subtitle = element_text(size = 10, hjust = 0.5, face = "italic")) +
        theme(plot.caption = element_text(size = 8, hjust = 0.0, face = "italic")) +
        theme(axis.title = element_text(size = 10)) +
        theme(axis.text.x = element_text(size = 14, color = "Black")) +
        theme(axis.text.y = element_text(size = 12, color = "Black")) +
        theme(legend.key.size = unit(0.5, 'cm')) +
        theme(legend.title = element_text(size=8)) +
        theme(legend.text = element_text(size=6))
```

One might argue that if participants are depreciating affective intensity but remembering their strategies correctly, this combination might be concealing an actual relationship. As such, I think we need to actually dive into the trajectories on an observation by observation basis. 

## How Memory of Regulated Events Change Over Time

I'm first going to create a subset of the data looking specifically at the memory of emotion regulation, so removing rows that were measuring emotion regulation at a delay. Simplest means of doing this might be subsetting two separate dataframes and then merging them based on PID and Event Number.

```{r}
df_mem <- subset(df, df$Timing == "MemReg", select = c("PID", "EventNum", "EmoMod", "Intense","Intense.z", "Valence", "Valence_z",
                                                               "Reg", "RegDesc", "RegSuccess", "RegSuccess.z", "EmoCat", "Distracted"))
df_imm <- subset(df, df$Timing == "ImmReg", select = c("PID", "EventNum","EmoMod", "Intense", "Intense.z", "Valence", "Valence_z",
                                                               "Reg", "RegDesc", "RegSuccess", "RegSuccess.z", "EmoCat", "Distracted"))
```

### QAing the New Dataframes

We're looking at the number of unique events in each dataframe.

```{r Checking Length of df_mem}
paste(df_mem$PID, df_mem$EventNum) %>%
                    unique() %>%
                    sort() %>%
                    length()
```

```{r Checking Length of df_imm}
paste(df_imm$PID, df_imm$EventNum) %>%
                    unique() %>%
                    sort() %>%
                    length()
```

The numbers don't match, so clearly there were at least some familiar recall events that were not classified as regulated using either reappraisal or distarction. It might also be the case that many of the 104 familiar recalls aren't events from the 120 immediate recordings, so I'll check content as well.

```{r For Loop to Find Missing Events}
  tracker = 0
  for (j in unique(paste(df_mem$PID, df_mem$EventNum))){
    if (!any(paste(df_imm$PID, df_imm$EventNum) == j)){
      # print(j)
      tracker = tracker + 1
    }
  }
  paste("Number of Events Without Comparison:", tracker)
```
 
 Ouff, so there are only 56 observations wherein a negative, downregulated, distraction-or-reappraisal regulated event was reported at both timepoints. This isn't even checking for whether the same details were reported at both. 

### Tracking Change in Emotional Intensity Between Timepoints

#### Recalling At Least One Emotion

We want to know how often are participants recalling at least one of their previously mentioned emotions, so I'll merge the dataframes leaving out any cases where emotions don't match exactly. Then I'll compare the length of the list of unique PID - Event combos between this and the full dataset.

```{r Recalled Event Dataframe}
df_compare <- merge(x = df_mem,
                    y = df_imm,
                    by = c("PID", "EventNum", "EmoMod", "Valence_z"),
                    all = FALSE)
```

```{r Number of Recalled Events}
Events_recalled <- paste(df_compare$PID, df_compare$EventNum) %>%
                    unique() %>%
                    sort() %>%
                    length()
  paste("Number of Recalled Events:", Events_recalled)
```

Only 39 observations across 29 unique events recalled the same emotion twice. That's pretty rough. 

```{r Number of Participants Who Recalled}
df_compare$PID %>%
  unique() %>%
  sort() %>%
  length() %>%
  paste("Number of Participants:", ., "| Percentage:", round(((./115)*100),2), "%")
```

It does not seem to be the case that there were people who remembered better than others. Not many folks who remembered correctly more than once it seems.

```{r Difference in affective intensity}
t.test(x = df_compare$Intense.z.x,
       y = df_compare$Intense.z.y,
       alternative = "two.sided",
       paired = T)
```

A t-test suggests that z-scored affective intensity reported later is significantly lower than initial reports (x = -0.283, t(38) = -3, p < 0.01)

```{r Difference in affective intensity}
df_compare$Intense.z_absdiff <- abs(df_compare$Intense.z.y - df_compare$Intense.z.x)
t.test(x = df_compare$Intense.z_absdiff,
       alternative = "two.sided",
       mu = 0)
```

Looking at the absolute difference of affective intensity scores over time suggests that the average difference is 0.386 standard deviation units ((t(38) = 4, p < 0.001)), reinforcing the analysis above. 

```{r Number of Observations in Which Strategy Matches}
df_compare$Congruent <- 0
df_compare$Congruent[df_compare$Distracted.x == df_compare$Distracted.y] <- 1
paste("Number of Congruent Observations:", sum(df_compare$Congruent), "| Percentage:", round(((sum(df_compare$Congruent)/nrow(df_compare))*100),2), "%")
```

And roughly 84.62% of observations were described similarly enough across time points to be categorized the same in terms of regulation strategy.

```{r Total Event Dataframe}
df_compare <- merge(x = df_mem,
                     y = df_imm,
                     by = c("PID", "EventNum", "EmoMod", "Valence_z"),
                     all = TRUE)
```

```{r Number of Total Events}
Events_total <- paste(df_compare$PID, df_compare$EventNum) %>%
                    unique() %>%
                    sort() %>%
                    length()
  paste("Number of Total Events:", Events_total)
```

```{r Percentage of Events Recalled}
((Events_recalled / Events_total) * 100) %>%
  round(2) %>%
  paste("Percent Events With An Emotion Recalled:", .,"%")
```

#### Recalling at Least One Emotion Category

I'm kind of curious if we get a little more liberal and just look for vaguer emotional categories whether we would see a significant increase. I'm thinking that I might be able to accurately recall that I had some generally positive experiences at a thing I did a week ago, but I might not remember exactly that I described it as "fantastic" or "content" or whatever.

```{r Recalled Event Dataframe}
df_compare <- merge(x = df_mem,
                     y = df_imm,
                     by = c("PID", "EventNum", "EmoCat"),
                     all = FALSE)
```

We see a nearly 50% improvement in our congruence across time points. Our observations climbed from 39 to 58. 

```{r Number of Recalled Events}
Events_recalled <- paste(df_compare$PID, df_compare$EventNum) %>%
                    unique() %>%
                    sort() %>%
                    length()
  paste("Number of Recalled Events:", Events_recalled)
```

```{r Number of Participants Who Recalled}
df_compare$PID%>%
  unique() %>%
  sort() %>%
  length() %>%
  paste("Number of Participants:", ., "| Percentage:", round(((./115)*100),2), "%")
```

```{r Difference in affective intensity}
t.test(x = df_compare$Intense.z.x,
       y = df_compare$Intense.z.y,
       alternative = "two.sided",
       paired = T)
```

A t-test suggests that z-scored affective intensity reported later are lower than than initial reports by an average of 0.294 standard deviation units (t(57) = -3, p = 0.01)

```{r Difference in affective intensity}
df_compare$Intense.z_absdiff <- abs(df_compare$Intense.z.y - df_compare$Intense.z.x)
t.test(x = df_compare$Intense.z_absdiff,
       alternative = "two.sided",
       mu = 0)
```

Looking at the absolute difference of affective intensity scores over time suggests that the average difference is 0.605 standard deviation units ((t(57) = 7, p < 0.001))

```{r Number of Observations in Which Strategy Matches}
df_compare$Congruent <- 0
df_compare$Congruent[df_compare$Distracted.x == df_compare$Distracted.y] <- 1
paste("Number of Congruent Observations:", sum(df_compare$Congruent), "| Percentage:", round(((sum(df_compare$Congruent)/nrow(df_compare))*100),2), "%")
```

Unfortunately, our number of congruent observations has reduced; 75.86% of observations were described similarly enough across time points to be categorized the same.

```{r Percentage of Events Recalled}
((Events_recalled / Events_total) * 100) %>%
  round(2) %>%
  paste("Percent Events With An Emotion Recalled:", .,"%")
```

```{R Cleaning Number of Emotion, echo = F, eval = T}
rm(j,df_compare,Events_recalled,df_imm, df_mem, tracker, Events_total)
```

## Constructing a Person-Level Dataframe
Many of the analyses that will follow are going to be looking at person level variables. As such, it is inappropriate to use a criterion variable that exists at the sub-participant level.

```{r Constructing a Person level Dataframe}
df_pl <- data.frame("PID" = unique(df$PID),
                    "PropDist" = NA)
df_pl <- subset(df, 
                !is.na(df$HH.Enjoy) , 
                select = c("PID", names(df)[grep(names(df), pattern = "Motive")],
                          "Pos.Anticipate", "Neg.Anticipate", "CogLoad_Post",
                          "Fear.Before", "Fear.During", "Intense.pmean.z", "Fear.Enjoy", "HH.Enjoy")) %>%
          distinct() %>%
          merge(x = df_pl,
                y = ., 
                by = "PID",
                all.x = F)

for (i in 1:nrow(df_pl)){
  total <- nrow(subset(df, df$PID == df_pl$PID[i]))
  dist <- nrow(subset(df, df$PID == df_pl$PID[i] & df$Distracted == 1))
  if (total > 0) {
    df_pl$PropDist[i] <- dist/total
  }
}

rm(i, total, dist)
```

## How Does Cognitive Load Infleunce Strategy Selection

Cognitive Load should predict strategy selection, in that a higher load predicts a greater likelihood of selecting reappraisal. However, we do not have enough data for models of the complexity I would have liked. Our models generally fail to converge with these covariates, so I am retroactively going back and adding this section wherein I explore the relationship between our covariates and variable of interest in isolation.

We measured cognitive load as the difference in RAT score between immediately after exposure and baseline. A score of 0 would suggest no difference, negative value correspond to reduce cognitive load and positive to increased cognitive load. 

```{r Models Using Cognitive Load}
summary(lm(PropDist ~ CogLoad_Post, data = df_pl))
summary(lm(PropDist ~ Intense.pmean.z + CogLoad_Post, data = df_pl))
```

We fail to find a model with predictive value in this relationship.

## Do Expectations Influence Strategy Choice? 

```{r Expectation Models}
summary(lm(PropDist ~ Pos.Anticipate + Neg.Anticipate + Fear.Before, data = df_pl))
```

It does not seem to be the case that the fear and negative emotion someone anticipates before hand significantly predicts proportion of strategy choice in these very specific circumstances.  

## Do Motivations Influence Strategy Choice?

```{r Motivation Models}
for (i in grep(names(df_pl), pattern = "Motive")){
  df_pl[,i] <- as.numeric(df_pl[,i])

}
summary(lm(PropDist ~ Motive.Payment + Motive.Thrill + Motive.Novelty + Motive.Challenge + Motive.Peers + Motive.Science + Motive.Bored, data = df_pl))
```

It looks like the motivation behind participating in the study, with the exception of boredom, did no have a significant effect upon strategy choice. Even in the case of boredom the effect size was negligible. 
 
## How Do Attitudes About Fear and Haunted Houses Influence Regulation? 

```{r Attitudes Towards Haunted Houses and Fear Models}
summary(lm(PropDist ~ HH.Enjoy + Fear.Enjoy + Fear.During, data = df_pl))
```

A one unit increase in fear indicated during the haunted house is associated with a 14.5% increase in the proportion of distraction to reappraisal usage. This would seem to support the relationship established in labs. We need to look at an event by event analysis, though, and it's also the case there are more negative emotions than just fear. 

```{r Follow Up Fear Model}
summary(lm(PropDist ~ HH.Enjoy + Fear.Enjoy * Fear.During, data = df_pl))
```

# Primary Analyses

Alright now on to our bread and butter. Therer was one question I was really stumped over for awhile and it was whether or not to include familiar recall in this analysis. On the one hand, they should ideally be exact replicas of what we observe in the immediate recall condition. On the other hand, they were categorized by our coders completely independent of one another, they were reported independent of one another, and the congruence between them is apparently incredibly low, even with the most liberal of criteria. As such, I think it makes sense to treat them as independent observations, especially since we can netter observe any timing effects that might be present.

## Affective Intensity

I did consider a model specifying Timing as a random slope, but we ran into convergence issues 

```{r Affective Intensity Models}
m0 <- glmer(Distracted ~ 1 + (1 | PID), data = df, family = binomial)
m1 <- glmer(Distracted ~ Intense.z + (1 | PID), data = df, family = binomial)
m2 <- glmer(Distracted ~ Intense.z + Timing + (1 | PID), data = df, family = binomial)
m3 <- glmer(Distracted ~ Intense.pc.z + Intense.pmean.z + (1 | PID), data = df, family = binomial)
m4 <- glmer(Distracted ~ Intense.pc.z + Intense.pmean.z + Timing + (1 | PID), data = df, family = binomial)
```

```{r Affective Intensity Model Comparisons}  
icc(m0)
anova(m0, m1)
anova(m1, m2)
anova(m2, m3)
anova(m1, m3)
anova(m3, m4)
```

```{r Affective Intensity Model Outputs}  
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
class(m3) <- "lmerMod"
class(m4) <- "lmerMod"
stargazer(m0,m1,m2,m3,m4,
          type = "text", 
          title= "Affect Predicting Regulation Usage", 
          column.labels = c("Null Model", "Model 1", "Model 2", "Model 3", "Model 4"),
          dep.var.labels = "Strategy Usage", 
          star.cutoffs = c(0.05, 0.01,0.001),
          covariate.labels = c("Affective Intensity (z)", "Recall, Familiar", "Recall, Novel", "Affective Intensity (Person-Centered, z)", "Affective Intensity (Person-Mean)", "Intercept"),
          ci = T,
          notes = "")
```

```{r Calculating Effect Sizes for Affective Intensity Models}
exp(fixef(m1))
exp(-0.084)
exp(0.561)
```

We fail to find a model with predictive value in this relationship.

```{r Visualizing Affective Models}
m1.e <- effect("Intense.z",m1,xlevels=list(Intense.z=seq(range(df$Intense.z)[1],
                                                       range(df$Intense.z)[2],
                                                       0.01))) 
m1.e <-as.data.frame(m1.e)
head(m1.e)
plot <- ggplot(m1.e, aes(x=Intense.z, y=fit)) +
  geom_jitter(data = df, aes(x=Intense.z, y=Distracted, 
                             color = as.factor(Distracted)), 
              width = 0.15, height = 0.025, size = 3.5) +
  geom_line(color = "Black", size = 4.5) +
  geom_ribbon(aes(ymin=lower, ymax=upper), alpha = 0.15) +
  scale_x_continuous("Emotion Intensity (z)", breaks = round(seq(range(df$Intense.z)[1],
                                                         range(df$Intense.z)[2], 
                                                         (range(df$Intense.z)[2] - range(df$Intense.z)[1])/6),0)) +
  scale_y_continuous("Probability of Using Distraction", breaks = seq(0,1,0.20)) +
  scale_color_manual(values = c("#F8766D", "#00BFC4")) +
  coord_cartesian(xlim = c(range(df$Intense.z)[1],
                           range(df$Intense.z)[2]), ylim = c(0,1)) +
  theme_classic() +  
  theme(legend.position = "none") +
  theme(axis.title = element_text(size = 32, color = "Black")) +
  theme(axis.text.x = element_text(size = 36, color = "Black")) +
  theme(axis.text.y = element_text(size = 30, color = "Black"))  
plot
```

```{r Exporting Regulation & Intensity Regression}
  tiff(paste0(Plots, "Fig5.tiff"),       
       res = 300,
       units = "in",
       width = 12, 
       height = 9)
  plot
  dev.off()
  rm(plot, m0,m1,m2,m3,m4,m5,m1.e)
```

# Follow Up Analyses
## Regulatory Success   

It would make sense that the interaction between affective intensity and regulatory usage should predict regulatory success. We should see greater regulatory success for low intensity, reappraisal events and high intensity distraction events

```{r Differences in Regulatory Success by Regulation Strategy}
t.test(df$RegSuccess.z[df$Distracted == 1],
       df$RegSuccess.z[df$Distracted == 0],
       alternative = "two.sided",
       paired = F)
```

Reappraisals were reported as more successful than distractions by nearly half of a standard deviation (xdiff = 0.529, t(394) = 6, p < 0.001)

```{r Regulation Success Models}
m0 <- lmer(RegSuccess.z ~ 1 + (1 | PID), data = df, REML = F)
m1 <- lmer(RegSuccess.z ~ Intense.z + (1 | PID), data = df, REML = F)
m2 <- lmer(RegSuccess.z ~ Intense.z + Distracted_cat + (1 | PID), data = df, REML = F)
m3 <- lmer(RegSuccess.z ~ Intense.z * Distracted_cat + (1 | PID), data = df, REML = F)
```

```{r Regulation Success Model Comparisons}  
icc(m0)
anova(m0, m1)
anova(m1, m2)
anova(m2, m3)
```

Our best performing model includes affective intensity and strategy as well as the interaction between the two (ICC = 0.423, p < 0.01)

```{r Model Outputs}  
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
class(m3) <- "lmerMod"
stargazer(m0,m1,m2,m3,
          type = "text", 
          title= "Affect & Strategy Predicting Regulation Success", 
          column.labels = c("Null Model", "Model 1", "Model 2", "Model 3"),
          dep.var.labels = "Regulation Success", 
          star.cutoffs = c(0.05, 0.01,0.001),
          covariate.labels = c("Affective Intensity (z)", "Strategy, Reappraised", "Intensity * Strategy", "Intercept"),
          ci = T,
          notes = "")
```

The interaction between strategy usage and affective intensity was deemed statistically significant (B 0.253, p < 0.01, 95% CI = [0.088, 0.418])

```{r Simple Slopes Analysis}
interactions::sim_slopes(model = m3, 
                         pred = Intense.z,
                         modx = Distracted_cat,
                         data = df,
                         confint = T)
```

A simple slopes analysis suggests that affective intensity is a significant predictor of regulation success for distracted, but not reappraised observations (B = -0.03, se = 0.07, t = -0.39, p > 0.05). However, the relationship is counter-intuitive: as affective intensity increases, the regulatory success of distraction decreases (B = -0.28, se = 0.06, t = -4.57, p < 0.01). This might make sense if reappraisal and distraction were switched, but a manual review of the data confirmed this was not the case. 

```{r Multilevel Model Plot}
df$fit <- fitted(m3)
plot <- ggplot(data = df, aes(x = Intense.z, y = fit, color = Distracted_cat)) +
                stat_smooth(method="lm", se = T, alpha = .6, size = 4.5) +
                scale_x_continuous(breaks = seq(-3,3,1)) +
                scale_y_continuous(breaks = seq(-2,2,1)) +
                scale_color_discrete(name = "Strategy", labels = c("Distraction", "Reappraisal")) +
                labs(x = "Emotion Intensity (z)", 
                     y ="Regulation Success (z)") +
                coord_cartesian(xlim=c(-2.5, 2.5), ylim=c(-2, 2)) +    
                theme_classic() +
                theme(legend.text = element_text(size = 24)) +
                theme(legend.title = element_text(size = 24)) +
                theme(legend.position = c(0.125, 0.125)) +
                theme(axis.title = element_text(size = 32)) +
                theme(axis.text.x = element_text(size = 36, color = "Black")) +
                theme(axis.text.y = element_text(size = 36, color = "Black")) 
plot
```

```{r Exporting Regulation & Intensity Regression}
  tiff(paste0(Plots, "Fig6.tiff"),       
       res = 300,
       units = "in",
       width = 12, 
       height = 9)
  plot
  dev.off()
  rm(plot)
```

```{r Multilevel Model 1 Coefficients}
  coef(m3)
```  

```{r Multilevel Model 1 Fixed Effects and Standard Error}  
  fixef(m3)
  se.fixef(m3)
```  

```{r Multilevel Model 1 Random Effects}  
  ranef(m3)
```  

```{r Multilevel Model 1 Descriptive Plots}
  plot_model(m3, type = "re")
  plot_model(m3, type = "pred")
  plot(m3)
```

``` {r Multilevel Model Assumption Tests}
  df$AbsResSqr <- (abs(residuals(m3)))^2
  LeveneTest <- lmer(AbsResSqr ~ Intense.z * Distracted_cat + (1 | PID), data=df, REML=F)
  anova(LeveneTest)
  qqmath(m2, id=0.05)
  rm(LeveneTest)
```  

```{r Cleaning Space}
rm(m0,m1,m2,m3)
```
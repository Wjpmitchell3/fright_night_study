---
title: "ER Forecast v. Usage - Study 1 Analysis"
author: "W.J. Mitchell"
date: "`r Sys.Date()`"
output: html_document
---

NAVIGATION / QUESTIONS: 
+ The pilot found that the strategies individuals described using approximately one week later were not related to the affective intensity of the experiences they underwent. However there were a number of limitations inherent to this study. For one, it's unclear how reliable the memory of emotion regulation strategy usage is one week after usage. It also might be the case that emotion regulation is not applied at the level of the event, which might contain multiple emotions, but at the level of the emotion. We also were left to infer the direction of regulation based upon descriptions. Lastly, there are a number of covariates which might be important to capture.

PREDICTIONS: 
+ We do not believe that negative affective intensity will continue to demonstrate predictive utility towards ER usage (i.e., distraction v. reappraisal) when measured beyond a lab context. We will adjust for a number of individual difference measures in the process.

ISSUES: 
+ We captured a number of covariates relevant to regulation strategy choice such as cognitive load and affective expectations. However, at the analysis stage, we found that we lacked the volume of data necessary to incorporate these variables into our models. As such we ran supplemental analyses, to measure their influence upon regulation strategy usage directly and found no association between cognitive load and strategy usage,

NOTES:

# Setup

```{r Packages I Use}
  # If the pacman package manager is not currently installed on this system, install it.
  if (require("pacman") == FALSE){
    install.packages("pacman")
  }

  # Loading in my packages with my pacman manager
  pacman::p_load(effects,
                 grid,
                 gridExtra,
                 here,
                 influence.ME,
                 lattice,
                 lme4, 
                 lmerTest,
                 multcomp,
                 performance,
                 sjPlot,
                 stargazer, 
                 tidyverse)
    
  # Loading a custom function that will clean NA values from dataframes
  source("https://github.com/wj-mitchell/stinkR/blob/main/remove_NAs.R?raw=TRUE", local = T)
  # Loading a custom function that will just make building blanke dataframes simpler
  source("https://github.com/wj-mitchell/stinkR/blob/main/make_df.R?raw=TRUE", local = T)
```

```{r Number Formatting}
  options(scipen=100)
  options(digits=3)
  options(tinytex.verbose = TRUE)
```

The `here()` function should ideally identify our upper-level project directory. So, `WorkDir` below should be a filepath that looks something like \`C:/Users/.../Documents/GitHub/fright_night_study/", or where ever you store these files. Then we'll have additional variables store filepaths within that directory for where we're reading the data from and where we're sending data to.

```{r Setting Working Directories}
  # Identifying our main directory
  WorkDir <- here::here()

  # Identifying specific directories to read and write from
  Import <- paste0(WorkDir, "/Data/Study_01/Raw/")
  Plots <- paste0(WorkDir, "/Products/Manuscripts/plots_tables/")
```

# Loading in the data

Once we've loaded in the data, we want to exclude any observations that lack a clear emotion (following lemmatization (i.e., `df$EmoMod`)). 

```{r Pulling in Dataframe}
  df <- read.csv(file = paste0(Import, "df.csv"),
                 header = T,
                 sep=",",
                 row.names = 1,
                 stringsAsFactors = F,
                 na.strings=c("","NA", "N/A"))

  df_all <- df %>%
        subset(!is.na(df$EmoMod) &
               df$EmoMod != "" &
               !is.na(df$PID))
```

Here we are again creating a binary variable to track whether any given observation was regulated with either strategy, but not both, and we are creating both a quantiative and qualitative variable.

```{r Creating a Disengagement Regulatory Superfactor}
df_all$Disengage <- NA
df_all$Disengage_cat <- NA
df_all$Disengage[(df_all$Attention.Deployment == 1 | df_all$Response.Modulation == 1) & df_all$Reappraisal == 0] <- 1
df_all$Disengage_cat[(df_all$Attention.Deployment == 1 | df_all$Response.Modulation == 1) & df_all$Reappraisal == 0] <- "Disengage"
df_all$Disengage[df_all$Attention.Deployment == 0 & df_all$Response.Modulation == 0 & df_all$Reappraisal == 1] <- 0
df_all$Disengage_cat[df_all$Attention.Deployment == 0 & df_all$Response.Modulation == 0 & df_all$Reappraisal == 1] <- "Engage"
```

and another that's ignoring cases of suppression to only focus on distraction and reappraisal.

```{r Creating a Regulatory Superfactor}
df_all$Distracted <- NA
df_all$Distracted_cat <- NA
df_all$Distracted[df_all$Attention.Deployment == 1 & df_all$Response.Modulation == 0 & df_all$Reappraisal == 0] <- 1
df_all$Distracted_cat[df_all$Attention.Deployment == 1 & df_all$Response.Modulation == 0 & df_all$Reappraisal == 0] <- "Distracted"
df_all$Distracted[df_all$Attention.Deployment == 0 & df_all$Response.Modulation == 0 & df_all$Reappraisal == 1] <- 0
df_all$Distracted_cat[df_all$Attention.Deployment == 0 & df_all$Response.Modulation == 0 & df_all$Reappraisal == 1] <- "Reappraised"
```

Unfortunately, in exporting and importing the .csv, some of the data is not in the proper structure anymore, so we need to fix that.

```{r Coercing data into the appropriate structure}
df_all$Intense.z <- as.numeric(df_all$Intense.z)
df_all$CogLoad_Post <- as.numeric(df_all$CogLoad_Post)
df_all$Intense.pmean.z <- as.numeric(df_all$Intense.pmean.z)
```

# DETOUR: Analyzing Strategy Use at the Section Level
Reviewer 4 expressed concern with utilizing idiosyncratic intensity to predict self-regulation, arguing that  we don't know if we captured intensity as a precursor or product of self-regulation. If we had normalized ratings for the different events in the haunted house, much as previous studies have relied on standardized image sets, then we wouldn't have this problme. We don't have normalized ratings, but we do have sections which are theoretically categorically different in terms of emotional intensity. The Haunted House was supposed to be organized into a more- and less-scary set of sections. If we aggregate across events and still find differences in strategy usage, it might suggest that our findings are still valid. I'll of course try to validate that these sections differ in terms of intensity first.

We asked subjects to rate from 1 to 5 how fearful they felt during each section immediately after the section ended. I'm using this to validate the categorization of each section. 

```{r Generating a New Dataframe of Unique Events}
# There was apparently an issue in which the box to describe event 3 was not appearing for some subjects so descriptions of it are missing. It affected four events, so we'll exclude those.
df_events <- df_all %>%
             subset(!is.na(.$EventDesc) & !is.na(df_all$Disengage)) %>%
             distinct(PID, EventNum, EventKey, EventDesc)

write.csv(df_events, paste0(WorkDir, "/data/Study_01/Raw/Unique_Events.csv"))
```

```{r Calculating Whether Regulation Differed By Section}
# Reading in the data
df_events <- read.csv(paste0(Import, "Unique_Events_Pilot_Final.csv"), na.strings = c("","-")) %>%
             rbind(., read.csv(paste0(Import, "Unique_Events_Primary_Final.csv"), na.strings = c("", "-")))

# Creating a dataframe of regulation activity and section where events occurred
df_temp <- merge(df_all,
                 y = df_events) %>%
           subset(.$Timing != "ImmReg" & !is.na(.$Distracted) & !is.na(.$Agreement),
                  select = c("PID", "EventNum", "Agreement", "Distracted")) %>%
           distinct() %>%
           mutate(Category = ifelse(Agreement %in% c("Delirium", "Take.13"), "Low", "High"))

# Generating a table to see how many events occurred in low and high intensity sections
table(df_temp$Category)

# Analyzing the relationship between intensity and strategy with a chi-square test
chisq.test(x = df_temp$Category,
           y = df_temp$Distracted)

# Examining the proportion of events regulated with each strategy by intensity. 
prop.table(table(df_temp$Distracted[df_temp$Category == "Low"]))
prop.table(table(df_temp$Distracted[df_temp$Category == "High"]))

```

```{r Finding whether there differences in average fear ratings by section type}
# Creating the dataframe
df_detour <- df_all %>% 
             distinct(PID, Crypt, Delirium, Take.13, Machine.Shop) %>%
             pivot_longer(cols = c(Crypt, Delirium, Take.13, Machine.Shop), 
                         names_to = "Section", 
                         values_to = "Rating") %>%
             mutate(Category = ifelse(Section %in% c("Delirium", "Take.13"), "Low", "High"))

# Constructing our t test
t.test(x = df_detour$Rating[df_detour$Category == "High"],
       y = df_detour$Rating[df_detour$Category == "Low"],
       paired = T)

# Calculating the mean
mean(df_detour$Rating[df_detour$Category == "High"], na.rm = T)
mean(df_detour$Rating[df_detour$Category == "Low"], na.rm = T)

# Cleaning our space
rm(df_detour)
```

```{r Checking Whether Emotions More Broadly Are Also More Intense By Section}
# Reading in the data
df_events <- read.csv(paste0(Import, "Unique_Events_Pilot_Final.csv"), na.strings = c("","-")) %>%
             rbind(., read.csv(paste0(Import, "Unique_Events_Primary_Final.csv"), na.strings = c("", "-")))

# Creating a dataframe of regulation activity and section where events occurred
df_temp <- merge(df_all,
                 y = df_events) %>%
           subset(!is.na(.$Distracted) & !is.na(.$Agreement),
                  select = c("PID", "EventNum", "Agreement", "Intense", "Valence")) %>%
           distinct() %>%
           mutate(Category = ifelse(Agreement %in% c("Delirium", "Take.13"), "Low", "High"))

# Constructing our t test
t.test(x = df_temp$Intense[df_temp$Category == "High"],
       y = df_temp$Intense[df_temp$Category == "Low"])

# Calculating the mean
mean(df_temp$Intense[df_temp$Category == "High"], na.rm = T)
mean(df_temp$Intense[df_temp$Category == "Low"], na.rm = T)
```

Okay, it does appear that, at least at the section level, the difference in-the-moment fear ratings is pretty substantial. 

# DETOUR DEUX: A Leave-One-Out Approach
Another way to address Reviewer 4's concerns about standardization with greater granularity might be a leave one out approach. We average the reported intensity of all subjects who experienced the event - except the experiencer - and use that to predict behavior.

We're going to reconstruct our dataset including the standardized event classifications
```{r Reading in the data}
# Creating a dataframe of regulation activity and events 
df_temp <- merge(df_all,
                 y = df_events) %>%
           subset(!is.na(.$Distracted) & !is.na(.$StandEvent) & !is.na(.$Agreement) 
                  & .$Reg == "Decrease" & .$Timing != "MemReg" & .$Valence <=0.5 ,
                  select = c("PID", "StandEvent", "Intense", 
                             "Distracted"))

# Identify PID and StandEvent combinations with differing Distracted values
differing_distracted <- df_temp %>%
                    group_by(PID, StandEvent) %>%
                    filter(n_distinct(Distracted) > 1) %>%
                    select(PID, StandEvent) %>%
                    distinct()

# Remove these combinations from the original dataframe - assuming that the average reported intensity for each event across downregulated negative emotions drives behavior
df_avg <- df_temp %>%
          anti_join(differing_distracted, by = c("PID", "StandEvent")) %>%
          group_by(PID, StandEvent) %>%
          mutate(Intense = mean(Intense)) %>%
          ungroup() %>%
          distinct()

# Creating a dataframe making a different assumption - that the maximum intensity drives behavior
df_max <- df_temp %>%
          anti_join(differing_distracted, by = c("PID", "StandEvent")) %>%
          group_by(PID, StandEvent) %>%
          filter(Intense == max(Intense)) %>%
          ungroup() %>%
          distinct()

# Filtering any events with fewer than three unique observations
df_avg <- df_avg %>% group_by(StandEvent) %>% filter(n() >= 3)
df_max <- df_max %>% group_by(StandEvent) %>% filter(n() >= 3)

# Generating a table to see how many events occurred in low and high intensity sections
all(table(df_avg$StandEvent) == table(df_max$StandEvent))
sort(table(df_avg$StandEvent))
mean(table(df_avg$StandEvent))
sd(table(df_avg$StandEvent))
median(table(df_avg$StandEvent))
range(table(df_avg$StandEvent))

# Computing the Leave One Out values
df_avg <- df_avg %>%
  group_by(StandEvent) %>%
  mutate(Intense.LOO = (sum(Intense) - Intense) / (n() - 1))

df_max <- df_max %>%
  group_by(StandEvent) %>%
  mutate(Intense.LOO = (sum(Intense) - Intense) / (n() - 1))
```

```{r Computing the analysis}
m_max <- glmer(Distracted ~ Intense.LOO + (1 | PID), data = df_max, family = binomial)
m_avg <- glmer(Distracted ~ Intense.LOO + (1 | PID), data = df_avg, family = binomial)
```

```{r Affective Intensity Model Outputs, warning=FALSE}
class(m_max) <- "lmerMod"
class(m_avg) <- "lmerMod"
stargazer(m_max,m_avg,
          type = "text", 
          title= "Affect Predicting Regulation Usage", 
          column.labels = c("Max Model", "Avg Model"),
          dep.var.labels = "Strategy Usage", 
          star.cutoffs = c(0.05, 0.01,0.001),
          # covariate.labels = c("Affective Intensity", "Intercept"),
          ci = T,
          notes = "")
```

and calculate our effect size and confidence intervals

```{r Calculating Effect Sizes for Affective Intensity Models}
exp(0.181)
exp(-1.580)
exp(1.950)

exp(0.361)
exp(-1.790)
exp(2.520)
```
```{r T-test for posterity's sake}
(t.test_max <- t.test(x = df_max$Intense.LOO[df_max$Distracted == 0],
                     y = df_max$Intense.LOO[df_max$Distracted == 1]))
(t.test_avg <- t.test(x = df_avg$Intense.LOO[df_avg$Distracted == 0],
                     y = df_avg$Intense.LOO[df_avg$Distracted == 1]))

```

We're still just not seeing an effect but we can get slightly more conservative
```{r Getting more conservative}
# Filtering any events with fewer than five unique observations
df_avg <- df_avg %>% group_by(StandEvent) %>% filter(n() >= 5)
df_max <- df_max %>% group_by(StandEvent) %>% filter(n() >= 5)

# Generating a table to see how many events occurred in low and high intensity sections
all(table(df_avg$StandEvent) == table(df_max$StandEvent))
sort(table(df_avg$StandEvent))
mean(table(df_avg$StandEvent))
sd(table(df_avg$StandEvent))
median(table(df_avg$StandEvent))
range(table(df_avg$StandEvent))

# Computing the Leave One Out values
df_avg <- df_avg %>%
  group_by(StandEvent) %>%
  mutate(Intense.LOO = (sum(Intense) - Intense) / (n() - 1))

df_max <- df_max %>%
  group_by(StandEvent) %>%
  mutate(Intense.LOO = (sum(Intense) - Intense) / (n() - 1))
```

```{r Computing the analysis}
m_max <- glmer(Distracted ~ Intense.LOO + (1 | PID), data = df_max, family = binomial)
m_avg <- glmer(Distracted ~ Intense.LOO + (1 | PID), data = df_avg, family = binomial)
```

```{r Affective Intensity Model Outputs, warning=FALSE}
class(m_max) <- "lmerMod"
class(m_avg) <- "lmerMod"
stargazer(m_max,m_avg,
          type = "text", 
          title= "Affect Predicting Regulation Usage", 
          column.labels = c("Max Model", "Avg Model"),
          dep.var.labels = "Strategy Usage", 
          star.cutoffs = c(0.05, 0.01,0.001),
          # covariate.labels = c("Affective Intensity", "Intercept"),
          ci = T,
          notes = "")
```

and calculate our effect size and confidence intervals

```{r Calculating Effect Sizes for Affective Intensity Models}
exp(-0.66)
exp(-2.860)
exp(2.730)

exp(-0.090)
exp(-3.350)
exp(3.170)
```

```{r T test for posterity's sake}
(t.test_max <- t.test(x = df_max$Intense.LOO[df_max$Distracted == 0],
                     y = df_max$Intense.LOO[df_max$Distracted == 1]))
(t.test_avg <- t.test(x = df_avg$Intense.LOO[df_avg$Distracted == 0],
                     y = df_avg$Intense.LOO[df_avg$Distracted == 1]))
```

# Subsetting Relevant Data

we're starting by focusing on those cases in which the intended regulation goal was to downregulate and excluding any cases in which we asked participants to recall emotional experiences that they previously recalled. We plan to reanalyze this data later to assess accuracy in regulatory recall. 

```{r Reducing Observations and Variables}
df_sub <- subset(df_all, 
             df_all$Reg == "Decrease" &
             df_all$Timing != "MemReg")
```

# Constructing a Person-Level Dataframe

Many of the analyses that will follow are going to be looking at person level variables. As such, it is inappropriate to use a criterion variable that exists at the sub-participant level (i.e., we want participant averaged values, like average intensity or proportion of strategies used).

```{r Constructing a Person level Dataframe}
# Creating a starting dataframe with a row for each PID and an empty column for strategy proportion
df_pl <- data.frame("PID" = unique(df_sub$PID),
                    "PropDist" = NA)

# Subsetting which columns we want to keep, removing NA rows, and merging it with the starting dataframe
df_pl <- subset(df_sub, 
                !is.na(df_sub$HH.Enjoy) , 
                select = c("PID", names(df_sub)[grep(names(df_sub), pattern = "Motive")],
                          "Pos.Anticipate", "Neg.Anticipate", "CogLoad_Post", "CogLoad_Pre", 
                          "Cond", "Date_HH", "Time_HH", "Group", "Fear.Before", "Fear.During", 
                          "Intense.pmean.z", "Fear.Enjoy", "HH.Enjoy", "Startle", "Peer.Presence",
                          "Sex", "Age", names(df_sub)[which(names(df_sub) == "BDI_Total"):which(names(df_sub) == "IRQ_PE")])) %>%
          distinct() %>%
          merge(x = df_pl,
                y = ., 
                by = "PID",
                all.x = F)

# Calculating the proportion of regulated events in which distraction is used
for (i in 1:nrow(df_pl)){
  total <- nrow(subset(df_sub, 
                       df_sub$PID == df_pl$PID[i]))
  dist <- nrow(subset(df_sub, 
                      df_sub$PID == df_pl$PID[i] & df_sub$Distracted == 1))
  if (total > 0) {
    df_pl$PropDist[i] <- dist/total
  }
}

# Cleaning our space
rm(i, total, dist)
```

# Possible Covariates

We'll start by analyzing a few covariates which could likely have an influence upon our effects.

## Timing's Influence on Regulation

First, related to previous limitations, I want to examine how affective intensity changes as a result of time of reporting. Joanne's paper suggests there may be a time effect, though, in her case it was due to an interaction effect that's a little more complicated than what I'm dealing with.

```{r Immediate versus Novel Contrast}
(model <- t.test(df_sub$Intense.z[df_sub$Timing == "ImmReg"],
                 df_sub$Intense.z[df_sub$Timing == "DelReg"],
                 alternative = "two.sided",
                 paired = F))
```

Yet interestingly the difference between immediate novel and delayed novel is not quite significant, or very large

```{r Reporting}
results <- report::report(model)
```

```{r Visualizing The Difference Between Novel Immediate and Delayed Recall}
df_sub$Timing <- factor(df_sub$Timing, levels = c("ImmReg", "DelReg"))
ggplot(data = df_sub, aes(x = Timing, y = Intense.z)) + 
       theme_classic()+
       geom_jitter(aes(alpha=0.5, color=Timing),shape=16, position=position_jitter(0.2)) + 
       geom_violin(trim=T, alpha=0.25) +
      labs(title = "Time Influenced Emotional Intensity",
                  subtitle = paste("No differences were observed between novel and familiar recalled events."),
                   x = NULL,
                   y ="Emotion Intensity") +
       scale_y_continuous(breaks = c(-2:2)) +
       coord_cartesian(ylim=c(-2.25,2.25)) +
       theme(legend.position="none") +
       geom_boxplot(width=0.2, color="black", alpha=0.0) +
       theme(plot.title = element_text(face="bold", size=8, hjust = 0.5)) +
       theme(plot.subtitle = element_text(size = 6, hjust = 0.5, face = "italic")) +
       theme(plot.caption = element_text(size = 8, hjust = 0.0, face = "italic")) +
       theme(axis.title = element_text(size = 14)) +
       theme(axis.text.x = element_text(size = 10, color = "Black", angle=-45, vjust = 1, hjust = 0.0)) +
       theme(axis.text.y = element_text(size = 12, color = "Black")) 
```

But more importantly, I want to know if timing is an important covariate to control for our outcome variable. I'm curious whether any differences exist in how frequently strategies were reported by timepoint.

```{r Frequency of Strategies at different time points}
  (model <- chisq.test(x = df_sub$Timing, y= df_sub$Distracted_cat))
  ggplot(data = subset(df_sub, !is.na(df_sub$Distracted_cat)), aes(x = Timing, color = Distracted_cat, fill = Distracted_cat)) +
        geom_bar() +
        scale_x_discrete("Timepoint", breaks = c("Immediate Recall", "Novel Recall")) +
        scale_y_continuous(breaks = c(0,100,200)) +
        labs(title = "Frequency of Strategy Usage by Time Reported",
            subtitle = "Strategies were reported equally as frequently across timepoints",
             x = NULL,
             y ="Frequency") +
        scale_color_brewer(palette = "Dark2") +
        scale_fill_brewer(palette = "Set2") +
        coord_cartesian(ylim=c(0.0, 200.0)) +
        theme_classic() +
        theme(plot.title = element_text(face="bold", size=8, hjust = 0.5)) +
        theme(plot.subtitle = element_text(size = 10, hjust = 0.5, face = "italic")) +
        theme(plot.caption = element_text(size = 8, hjust = 0.0, face = "italic")) +
        theme(axis.title = element_text(size = 10)) +
        theme(axis.text.x = element_text(size = 14, color = "Black")) +
        theme(axis.text.y = element_text(size = 12, color = "Black")) +
        theme(legend.key.size = unit(0.5, 'cm')) +
        theme(legend.title = element_text(size=8)) +
        theme(legend.text = element_text(size=6))
```

We also fail to find any significant differences in strategy usage as a consequence of the timepoint at which they were reported.

```{r Reporting}
results <- c(results, "A Person's chi-squared test suggests that no differences exist in the endorsement of strategy usage by timepoint (x2 = 0.001, df = 1, p = 1)")
```

## How Does Cognitive Load Infleunce Strategy Selection

Cognitive Load should predict strategy selection, in that a higher load predicts a greater likelihood of selecting reappraisal. However, we do not have enough data for models of the complexity I would have liked. Our models generally fail to converge with these covariates, so I am retroactively going back and adding this section wherein I explore the relationship between our covariates and variable of interest in isolation.

We measured cognitive load as the difference in RAT score between immediately after exposure and baseline. A score of 0 would suggest no difference, negative value correspond to reduce cognitive load and positive to increased cognitive load.

```{r Models Using Cognitive Load}
summary(lm(PropDist ~ CogLoad_Post, data = df_pl))
confint(lm(PropDist ~ CogLoad_Post, data = df_pl))
summary(lm(Intense.pmean.z ~ CogLoad_Post, data = df_pl))
summary(lm(PropDist ~ Intense.pmean.z + CogLoad_Post, data = df_pl))
summary(lm(PropDist ~ Intense.pmean.z * CogLoad_Post, data = df_pl))
summary(lm(PropDist ~ CogLoad_Pre, data = df_pl))
summary(lm(Intense.pmean.z ~ CogLoad_Pre, data = df_pl))
summary(lm(PropDist ~ Intense.pmean.z + CogLoad_Pre, data = df_pl))
summary(lm(PropDist ~ Intense.pmean.z * CogLoad_Pre, data = df_pl))
```

We fail to find a model with predictive value in this relationship as well.

```{r Results}
results <- c(results, "When examining the association between cognitive load as measured immediately after the haunted house relative to each participant's baseline cognitive load and affective intensity, we fail to find any significant association (b = 0.00, se = 0.05, t(74) = 0.04, p = 0.97). When examining cognitive load's predictive utility towards strategy usage, we also fail to find any significant association (b = -0.01, se = 0.01, t(74) = -1.26, p = 0.21), even when adjusting for affective intensity within the model (b = -0.02, se = 0.01, t(73) = -1.27, p = 0.21).")
```

## Do Expectations Influence Strategy Choice?

We would expect that the expectation that participants enter the haunted house with would likely have an influence upon their experience when in the haunted house.

```{r Expectation Models}
summary(lm(PropDist ~ Pos.Anticipate + Neg.Anticipate + Fear.Before, data = df_pl))
confint(lm(PropDist ~ Pos.Anticipate + Neg.Anticipate + Fear.Before, data = df_pl))
summary(lm(Intense.pmean.z ~ Pos.Anticipate + Neg.Anticipate + Fear.Before, data = df_pl))
```

It does not seem to be the case that the fear and negative emotion someone anticipates before hand significantly predicts proportion of strategy choice in these very specific circumstances. However, the intensity of affective experiences was strongly predicted by both positive and negative anticipation.

```{r Results, warning=FALSE}
results <- c(results, 
             report::report(lm(PropDist ~ Pos.Anticipate + Neg.Anticipate + Fear.Before, data = df_pl)),
             report::report(lm(Intense.pmean.z ~ Pos.Anticipate + Neg.Anticipate + Fear.Before, data = df_pl)))
```

## Do Motivations Influence Strategy Choice?

Similarly, the motivation one enters the haunted house for might influence their experience as well.

```{r Motivation Models}
for (i in grep(names(df_pl), pattern = "Motive")){
  df_pl[,i] <- as.numeric(df_pl[,i])

}
model1 <- lm(PropDist ~ Motive.Payment + Motive.Thrill + Motive.Novelty + Motive.Challenge + Motive.Peers + Motive.Science + Motive.Bored, data = df_pl)
confint(model1)
summary(model1)
model2 <- lm(Intense.pmean.z ~ Motive.Payment + Motive.Thrill + Motive.Novelty + Motive.Challenge + Motive.Peers + Motive.Science + Motive.Bored, data = df_pl)
summary(model2)
```

It looks like the motivation behind participating in the study did no have a significant effect upon strategy choice. Even in the case of boredom the effect size was negligible.

```{r Results, warning=FALSE}
results <- c(results, 
             report::report(model1),
             report::report(model2))
```


```{r Contrast Dataframes}
# Creating a dataframe for contrasts
df_motive <- df_pl[,c(1,3:9)] %>%
             pivot_longer(cols = starts_with("Motive"),
                          values_to = "Value",
                          names_to = "Motives")

# Restructuring our variables
df_motive$Motives <- as.factor(df_motive$Motives)
```

```{r Contrast Parent Model}

# Creating an ANOVA model
aov <- aov(Value ~ Motives, data = df_motive)
summary(aov)
```

```{r Contrasts}
# Define contrasts (here's an example for three groups)
contrasts <- matrix(c(-1,  0,  0,  0,  0,  0, 1,
                       0, -1,  0,  0,  0,  0, 1,
                       0,  0, -1,  0,  0,  0, 1,
                       0,  0,  0, -1,  0,  0, 1,
                       0,  0,  0,  0, -1,  0, 1,
                       0,  0,  0,  0,  0, -1, 1,
                      -1, -1, -1, -1, -1, -1, 6),
                    byrow = TRUE, nrow = 7)

# Apply the contrasts using glht
comparison <- glht(aov, linfct = mcp(Motives = contrasts))

# Adjust p-values using Bonferroni method and display the results
summary(comparison, test = adjusted("bonferroni"))
```


## How Do Attitudes About Fear and Haunted Houses Influence Regulation?

We assessed the extent to which people enjoy fear and haunted houses. I have a suspicion that they'll be pretty strongly correlated.

```{r Overlap in Constructs}
(model <- cor.test(df_all$HH.Enjoy, 
                   df_all$Fear.Enjoy,
                   use = "everything"))
```

and we find that to be the case.

```{r Results}
results <- c(results, 
             report::report(model))
```

We aren't really finding anything significant in the main effects models, a little bit of `fear.during` predicting the average itensity participants report. 

```{r Attitudes Towards Haunted Houses and Fear Models}
summary(lm(PropDist ~ HH.Enjoy + Fear.Enjoy + Fear.During, data = df_pl))
confint(lm(PropDist ~ HH.Enjoy + Fear.Enjoy + Fear.During, data = df_pl))
summary(lm(Intense.pmean.z ~ HH.Enjoy + Fear.Enjoy + Fear.During, data = df_pl))
```

However, we are finding a pretty notable interaction between how much someone enjoys fear and their fear during the haunted house upon how intense their emotions were. While this demonstrates no predictive utility towards strategy again, it could constitute an important covariate to our predictor. 

```{r Follow Up Fear Model}
model1 <- lm(PropDist ~ HH.Enjoy + Fear.Enjoy * Fear.During, data = df_pl)
summary(model1)
model2 <- lm(Intense.pmean.z ~ HH.Enjoy + Fear.Enjoy * Fear.During, data = df_pl)
summary(model2)
```

```{r Results, warning=FALSE}
results <- c(results, 
             report::report(model1),
             report::report(model2))
```

## How Do Conventional Demographics Influence Regulation?

A sizeable proportion of literature finds that men use suppression more often and reappraisal usage increases with age. We're going to see whether we have cause for concern with these variables in our dataset. It should be noted that we likely do not have enough older adults to really see the effects I mentioned. 

We are actually seeing a slightly negative effect between being male and have a more intense experience, but again, nothing to our outcome variable. 

```{r Follow Up Demographics Model}
model1 <- lm(PropDist ~ Age + Sex, data = df_pl)
summary(model1)
confint(model1)
model2 <- lm(Intense.pmean.z ~ Age + Sex, data = df_pl)
summary(model2)
```

```{r Results, warning=FALSE}
results <- c(results, 
             report::report(model1),
             report::report(model2))
```

## How does the presence of peers influence Regulation?

We find no difference in intensity or our outcome variable between individuals who knew others in their group and who did not know others in their group. 

```{r Follow Up Peer Presence Models}
(model1 <- t.test(df_pl$PropDist[df_pl$Peer.Presence == "Yes"],
                  df_pl$PropDist[df_pl$Peer.Presence == "No"]))
(model2 <- t.test(df_pl$Intense.pmean.z[df_pl$Peer.Presence == "Yes"],
                  df_pl$Intense.pmean.z[df_pl$Peer.Presence == "No"]))
```

```{r Results}
results <- c(results, 
             report::report(model1),
             report::report(model2))
```

## How do various other individual difference measures influence Regulation?

Here, we're going to test a bevy of other validated individual difference measures to see if *anything* is correlating with our outcome and constituting a concerning covariate. 

```{r Identifying columns containing covariate data}
cols <- c(which(names(df_pl) == "BDI_Total"):which(names(df_pl) == "IRQ_PE"))
```

```{r Correlating Individual Differences With Predictor}
# Iterating through each of our potential covariates
for (VAR in 1:(length(cols))){
  
  # Pulling up our outcome variable 
  item <- df_pl[,which(names(df_pl) == "Intense.pmean.z")] %>%
          # Converting the data to numeric from double
          unlist() %>%
          # Running a correlation test between the covariate and outcome
          cor.test(x = .,
                   y = df_pl[,cols[VAR]],
                   use = "pairwise.complete.obs")
  
  # If the given correlation is significant
  if (item$p.value < 0.05){
    # Report the statistics
    print(paste0(names(df_pl)[cols[VAR]], 
                 " demonstrates a significant association to our outcome variable (t = ",
                 round(item$statistic, 3), ", r = ", round(item$estimate, 3), ", p = ", round(item$p.value, 3),")"))
  }

  # If the given correlation is non-significant  
  if (item$p.value >= 0.05){
    #Report the statistics
    print(paste0(names(df_pl)[cols[VAR]], 
                 ": NO SIGNIFICANT ASSOCIATION w/ OUTCOME (t = ",
                 round(item$statistic,3), ", r = ", round(item$estimate,3), ", p = ", round(item$p.value,3),")"))
  }
}

# Clean our space 
rm(item, VAR)
```

```{r Correlating Individual Differences With Outcomes}
# Iterating through each of our potential covariates
for (VAR in 1:(length(cols))){
  
  # Pulling up our outcome variable 
  item <- df_pl[,which(names(df_pl) == "PropDist")] %>%
          # Converting the data to numeric from double
          unlist() %>%
          # Running a correlation test between the covariate and outcome
          cor.test(x = .,
                   y = df_pl[,cols[VAR]],
                   use = "pairwise.complete.obs")
  
  # If the given correlation is significant
  if (item$p.value < 0.05){
    # Report the statistics
    print(paste0(names(df_pl)[cols[VAR]], 
                 " demonstrates a significant association to our outcome variable (t = ",
                 round(item$statistic, 3), ", r = ", round(item$estimate, 3), ", p = ", round(item$p.value, 3),")"))
  }

  # If the given correlation is non-significant  
  if (item$p.value >= 0.05){
    #Report the statistics
    print(paste0(names(df_pl)[cols[VAR]], 
                 ": NO SIGNIFICANT ASSOCIATION w/ OUTCOME (t = ",
                 round(item$statistic,3), ", r = ", round(item$estimate,3), ", p = ", round(item$p.value,3),")"))
  }
}

# Clean our space 
rm(item, VAR, cols)
```

## How does date, time, or group influence Regulation?

Lastly, we're going to explore the influence of variance time factors, like which group people were in, which time they entered the haunted house, and the dates on which they did. 

```{r Running an ANOVA to test the effect of Time}
model1 <- aov(PropDist ~ Time_HH, data=df_pl)
summary(model1)
model2 <- aov(Intense.pmean.z~ Time_HH, data=df_pl)
summary(model2)
```

```{r Results, warning=FALSE, warning=FALSE}
results <- c(results, 
             report::report(model1),
             report::report(model2))
```

```{r Running an ANOVA to test the effect of date}
model1 <- aov(PropDist ~ Date_HH, data=df_pl)
summary(model1)
model2 <- aov(Intense.pmean.z~ Date_HH, data=df_pl)
summary(model2)
```

```{r Results, warning=FALSE, warning=FALSE}
results <- c(results, 
             report::report(model1),
             report::report(model2))
```

```{r Running an ANOVA to test the effect of group}
model1 <- aov(PropDist ~ Group, data=df_pl)
summary(model1)
model2 <- aov(Intense.pmean.z~ Group, data=df_pl)
summary(model2)
```

```{r Results, message=FALSE, warning=FALSE}
results <- c(results, 
             report::report(model1),
             report::report(model2))
```


# Primary Analyses

As mentioned within the pilot script, I think I need to be as charitable to the hypothesis that intensity will predict strategy as possible. Thus I will test this relationship in as many ways as I can theoretically justify to find the exact conditions in which it makes sense. All of these analyses will be using hierarchical binary logistic regression. I've covered why that's important and my approach to it in the pilot script as well, so I won't overwhelm you, brave reviewer, with fine-grained details. Broadly, in each case, I am taking an information criterion approach and adopting the model that best fits the data as determined via chi-square model comparison of AIC values. These models will be built iteratively from a null model featuring only random effects and will stop once an iteration fails to perform better than the model before. The one exception to this rule is that I will always test the full model with covariates included since that is what I originally hypothesized and pre-registered. 

## Application of Engagement/Disengagement Strategies with Positive and Negative Emotions

We are starting off with the broadest inclusion criteria, so we're including all observations in which an engagement or disengagement strategy was used for an emotional experience, whether it was positive or negative. This might include *too* much variance, but again, I want to make as few assumptions as possible in testing these models and let the data speak for itself.

We're starting by subsetting the relevant variables and data

```{r Creating a Subset of Data}
df <- subset(df_sub, 
             !is.na(df_sub$Disengage),
             select = c("Disengage", "PID", "Intense", 
                        "Intense.z", "Intense.pc.z", "Intense.pmean.z", "Timing", "EventNum", "Fear.Before", "Fear.Enjoy", "ERQ_CogReap"))
```

Now I'm printing summary statistics

```{r Printing Details}
paste("Number of Participants:", length(unique(df$PID)))
paste("Number of Events:", nrow(distinct(subset(df, select = c("PID", "Timing", "EventNum")))))
paste("Number of Immediate Events:", nrow(distinct(subset(df, df$Timing == "ImmReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Familiar Recall Events:", nrow(distinct(subset(df, df$Timing == "MemReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Novel Recall Events:", nrow(distinct(subset(df, df$Timing == "DelReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Disengagement cases:", length(which(df$Disengage == 1)))
paste("Number of Engagement cases:", length(which(df$Disengage == 0)))
```

Now I'm constructing a null model without fixed effects, constructing two models that approach the question from a person-centered and non-person-centered approach, and a final model that includes covariates.

```{r Affective Intensity Models}
m0 <- glmer(Disengage ~ 1 + (1 | PID), data = df, family = binomial)
m1 <- glmer(Disengage  ~ Intense.z + (1 | PID), data = df, family = binomial)
m2 <- glmer(Disengage  ~ Intense.pc.z + Intense.pmean.z + (1 | PID), data = df, family = binomial)
mcov <- glmer(Disengage  ~ Intense.z + Fear.Before + Fear.Enjoy + ERQ_CogReap + (1 | PID), data = df, family = binomial)
```

Then, I'm testing the ICC value of my null model to examine whether using a hierarchical approach can be justified both theoretically *and* via the data generated (Again, discussed in my script for the pilot, but we ideally want to see an ICC > 0.0). Then I'm testing both of my models against the null model to see whether they explain a greater degree of variance while adjusting for the number of predictors than does the null model. I explained this in the pilot script also, but we ideally never want to compare a model with *n* terms to any other model with *n+2* or *n-2* terms. This would generally be an inappropriate comparison because comparing models with such vastly different term quantities would be a little like apples to oranges and would mask *why* one model performs better than the other. However, `m2` in this case is effectively the same predictor used in `m1` just divorced into two separate components. We are not explaining any more variance by introducing a separate term; we're just examining whether the relationship between the predictor and the outcome changes if we arrange it in a slightly more subjective way. As such, I think such an approach is justified in this instance.  

```{r Affective Intensity Model Comparisons}
icc(m0)
anova(m0, m1)
anova(m0, m2)
```

Following model comparison, we generate a results table...

```{r Affective Intensity Model Outputs, warning=FALSE}
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
class(mcov) <- "lmerMod"
stargazer(m0,m1,m2,mcov,
          type = "text", 
          title= "Affect Predicting Regulation Usage", 
          column.labels = c("Null Model", "Model 1", "Model 2"),
          dep.var.labels = "Strategy Usage", 
          star.cutoffs = c(0.05, 0.01,0.001),
          covariate.labels = c("Affective Intensity (z)", "Affective Intensity (Person-Centered, z)", 
                               "Affective Intensity (Person-Mean)", "Fear Expectation" , "Fear Enjoy" , 
                               "ERQ (Reappraisal)" , "Intercept"),
          ci = T,
          notes = "")
```

and calculate our effect size and confidence intervals

```{r Calculating Effect Sizes for Affective Intensity Models}
exp(fixef(m1))
exp(-0.052)
exp(0.538)

exp(fixef(mcov))
exp(-0.068)
exp(0.548)
```

Again, we're going to repeat this process throughout so I'm going to scale back the commentary


## Application of Engagement/Disengagement Strategies with Negative Emotions

Now, we're going to narrow our scope a tiny bit and see if the relationship emerges when we only examine negative emotions. This more closely mirrors what we see in most lab studies, which tend to care about hedonic responses to negative emotions (i.e., reducing them). We're not quite at downregualtion yet, though. 

```{r Creating a Subset of Data}
df <- subset(df_sub, 
             !is.na(df_sub$Disengage) &
             df_sub$Valence < 0.5,
             select = c("Disengage", "PID", "Intense.z", "Intense.pc.z", "Intense.pmean.z", "Timing", "EventNum", "Fear.Before", "Fear.Enjoy", "ERQ_CogReap"))
```

```{r Printing Details}
paste("Number of Participants:", length(unique(df$PID)))
paste("Number of Events:", nrow(distinct(subset(df, select = c("PID", "Timing", "EventNum")))))
paste("Number of Immediate Events:", nrow(distinct(subset(df, df$Timing == "ImmReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Familiar Recall Events:", nrow(distinct(subset(df, df$Timing == "MemReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Novel Recall Events:", nrow(distinct(subset(df, df$Timing == "DelReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Disengagement cases:", length(which(df$Disengage == 1)))
paste("Number of Engagement cases:", length(which(df$Disengage == 0)))
```

```{r Affective Intensity Models}
m0 <- glmer(Disengage ~ 1 + (1 | PID), data = df, family = binomial)
m1 <- glmer(Disengage  ~ Intense.z + (1 | PID), data = df, family = binomial)
m2 <- glmer(Disengage  ~ Intense.pc.z + Intense.pmean.z + (1 | PID), data = df, family = binomial)
mcov <- glmer(Disengage  ~ Intense.z + Fear.Before + Fear.Enjoy + ERQ_CogReap + (1 | PID), data = df, family = binomial)
```

```{r Affective Intensity Model Comparisons}
icc(m0)
anova(m0, m1)
anova(m0, m2)
```

```{r Affective Intensity Model Outputs, warning=FALSE}
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
class(mcov) <- "lmerMod"
stargazer(m0,m1,m2,mcov,
          type = "text", 
          title= "Affect Predicting Regulation Usage", 
          column.labels = c("Null Model", "Model 1", "Model 2"),
          dep.var.labels = "Strategy Usage", 
          star.cutoffs = c(0.05, 0.01,0.001),
          covariate.labels = c("Affective Intensity (z)", "Affective Intensity (Person-Centered, z)", 
                               "Affective Intensity (Person-Mean)", "Fear Expectation" , "Fear Enjoy" , 
                               "ERQ (Reappraisal)" , "Intercept"),
          ci = T,
          notes = "")
```

```{r Calculating Effect Sizes for Affective Intensity Models}
exp(fixef(m1))
exp(-0.161)
exp(0.488)

exp(fixef(mcov))
exp(-0.185)
exp(0.494)
```

## Application of Reappraisal/Distract Strategies with Positive and Negative Emotions

Now we're narrowing things a little bit further in that we aren't examining the contrast between engagement and disengagement strategies, but instead specifically between reappraisal and distraction, which are the two specific strategies that are most commonly compared in the aforementioned lab studies. 

```{r Creating a Subset of Data}
df <- subset(df_sub, 
             !is.na(df_sub$Distracted),
             select = c("Distracted", "PID", "Intense.z", "Intense.pc.z", "Intense.pmean.z", "Timing", "EventNum", "Fear.Before", "Fear.Enjoy", "ERQ_CogReap"))
```

```{r Printing Details}
paste("Number of Participants:", length(unique(df$PID)))
paste("Number of Events:", nrow(distinct(subset(df, select = c("PID", "Timing", "EventNum")))))
paste("Number of Immediate Events:", nrow(distinct(subset(df, df$Timing == "ImmReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Familiar Recall Events:", nrow(distinct(subset(df, df$Timing == "MemReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Novel Recall Events:", nrow(distinct(subset(df, df$Timing == "DelReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Distracted cases:", length(which(df$Distracted == 1)))
paste("Number of Engagement cases:", length(which(df$Distracted == 0)))
```

```{r Affective Intensity Models}
m0 <- glmer(Distracted ~ 1 + (1 | PID), data = df, family = binomial)
m1 <- glmer(Distracted  ~ Intense.z + (1 | PID), data = df, family = binomial)
m2 <- glmer(Distracted  ~ Intense.pc.z + Intense.pmean.z + (1 | PID), data = df, family = binomial)
mcov <- glmer(Distracted  ~ Intense.z + Fear.Before + Fear.Enjoy + ERQ_CogReap + (1 | PID), data = df, family = binomial)
```

```{r Affective Intensity Model Comparisons}
icc(m0)
anova(m0, m1)
anova(m0, m2)
```

```{r Affective Intensity Model Outputs, warning=FALSE}
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
class(mcov) <- "lmerMod"
stargazer(m0,m1,m2,mcov,
          type = "text", 
          title= "Affect Predicting Regulation Usage", 
          column.labels = c("Null Model", "Model 1", "Model 2"),
          dep.var.labels = "Strategy Usage", 
          star.cutoffs = c(0.05, 0.01,0.001),
          covariate.labels = c("Affective Intensity (z)", "Affective Intensity (Person-Centered, z)", 
                               "Affective Intensity (Person-Mean)", "Fear Expectation" , "Fear Enjoy" , 
                               "ERQ (Reappraisal)" , "Intercept"),
          ci = T,
          notes = "")
```

```{r Calculating Effect Sizes for Affective Intensity Models}
exp(fixef(m1))
exp(0.025)
exp(0.683)

exp(fixef(mcov))
exp(0.028)
exp(0.719)
```

## Application of Reappraisal/Distract Strategies with Negative Emotions

Now we're going to narrow this further by specifically looking at reappraisal and distraction within negative emotions only. 

```{r Creating a Subset of Data}
df <- subset(df_sub, 
             !is.na(df_sub$Distracted) &
             df_sub$Valence < 0.5,
             select = c("Distracted", "Distracted_cat", "PID", "Intense", "EmoMod", "Intense.z", "Intense.pc.z", "Intense.pmean.z", "Timing", "EventNum", "Fear.Before", "Fear.Enjoy", "ERQ_CogReap", "RegSuccess", "RegSuccess.z"))
```

```{r Printing Details}
paste("Number of Participants:", length(unique(df$PID)))
paste("Number of Events:", nrow(distinct(subset(df, select = c("PID", "Timing", "EventNum")))))
paste("Number of Immediate Events:", nrow(distinct(subset(df, df$Timing == "ImmReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Familiar Recall Events:", nrow(distinct(subset(df, df$Timing == "MemReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Novel Recall Events:", nrow(distinct(subset(df, df$Timing == "DelReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Distracted cases:", length(which(df$Distracted == 1)))
paste("Number of Engagement cases:", length(which(df$Distracted == 0)))
```

```{r Affective Intensity Models}
m0 <- glmer(Distracted ~ 1 + (1 | PID), data = df, family = binomial)
m1 <- glmer(Distracted  ~ Intense + (1 | PID), data = df, family = binomial)
m2 <- glmer(Distracted  ~ Intense.pc.z + Intense.pmean.z + (1 | PID), data = df, family = binomial)
mcov <- glmer(Distracted  ~ Intense.z + Fear.Before + Fear.Enjoy + ERQ_CogReap + (1 | PID), data = df, family = binomial)
```

```{r Affective Intensity Model Comparisons}
icc(m0)
anova(m0, m1)
anova(m0, m2)
```

```{r Affective Intensity Model Outputs, warning=FALSE}
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
class(mcov) <- "lmerMod"
stargazer(m0,m1,m2,mcov,
          type = "text", 
          title= "Affect Predicting Regulation Usage", 
          column.labels = c("Null Model", "Model 1", "Model 2"),
          dep.var.labels = "Strategy Usage", 
          star.cutoffs = c(0.05, 0.01,0.001),
          covariate.labels = c("Affective Intensity (z)", "Affective Intensity (Person-Centered, z)", 
                               "Affective Intensity (Person-Mean)", "Fear Expectation" , "Fear Enjoy" , 
                               "ERQ (Reappraisal)" , "Intercept"),
          ci = T,
          notes = "")
```

```{r Calculating Effect Sizes for Affective Intensity Models}
exp(fixef(m1))
exp(-0.056)
exp(0.668)

exp(fixef(mcov))
exp(-0.052)
exp(0.689)
```


```{r Visualizing Affective Models}
set.seed(1234)
m1.e <- effect("Intense",m1,xlevels=list(Intense=seq(range(df$Intense)[1],
                                                       range(df$Intense)[2],
                                                       0.01))) 
m1.e <-as.data.frame(m1.e)
head(m1.e)
plot2a_online <- ggplot(m1.e, aes(x=Intense, y=fit)) +
  geom_jitter(data = df, alpha = 0.35, size = 3.0, aes(x=Intense, y=Distracted, 
                             color = as.factor(Distracted)), 
              width = 0.3, height = 0.04) +
  geom_line(color = "Black", size = 3.0) +
  geom_ribbon(aes(ymin=lower, ymax=upper), alpha = 0.15) +
  scale_x_continuous("Emotion Intensity") +
  scale_y_continuous("Probability of Using Distraction", breaks = seq(0,1,0.20)) +
  scale_color_manual(values = c("#00BFC4", "#F8766D"), labels = c("Reappraisal", "Distraction"), name = "Strategy") +
  coord_cartesian(xlim = c(0, 4), ylim = c(0,1)) +
  theme_classic() +
  theme(axis.title = element_text(size = 24, color = "Black", face = "bold"),
        axis.text.x = element_text(size = 24, color = "Black"),
        axis.text.y = element_text(size = 24, color = "Black"), 
        legend.title = element_text(size = 24, face = "bold"),
        legend.text = element_text(size = 18),
        legend.position = c(0.200, .780))  
plot2a_online
```

```{r Exporting Regulation & Intensity Regression}
  tiff(paste0(Plots, "archive/Fig2a_online.tiff"),       
       res = 1200,
       units = "in",
       width = 6.5, 
       height = 6.0)
  plot2a_online
  dev.off()
```

```{r Visualizing Affective Models}
set.seed(1234)
m1.e <- effect("Intense",m1,xlevels=list(Intense=seq(range(df$Intense)[1],
                                                       range(df$Intense)[2],
                                                       0.01))) 
m1.e <-as.data.frame(m1.e)
head(m1.e)
plot2a_print <- ggplot(m1.e, aes(x=Intense, y=fit)) +
  geom_jitter(data = df, alpha = 0.35, size = 3.0, aes(x=Intense, y=Distracted, 
              color = as.factor(Distracted)), 
              width = 0.3, height = 0.04) +
  geom_line(color = "Black", size = 3.0) +
  geom_ribbon(aes(ymin=lower, ymax=upper), alpha = 0.15) +
  scale_x_continuous("Emotion Intensity") +
  scale_y_continuous("Probability of Using Distraction", breaks = seq(0,1,0.20)) +
  scale_color_manual(values = c("gray20", "gray60"), labels = c("Reappraisal", "Distraction"), name = "Strategy") +
  coord_cartesian(xlim = c(0, 4), ylim = c(0,1)) +
  theme_classic() +  
  theme(axis.title = element_text(size = 24, color = "Black", face = "bold"),
        axis.text.x = element_text(size = 24, color = "Black"),
        axis.text.y = element_text(size = 24, color = "Black"), 
        legend.title = element_text(size = 24, face = "bold"),
        legend.text = element_text(size = 18),
        legend.position = c(0.200, .780))  
plot2a_print
```

```{r Exporting Regulation Success & Intensity Regression}
  tiff(paste0(Plots, "archive/Fig2a_print.tiff"),       
       res = 1200,
       units = "in",
       width = 6.0, 
       height = 6.0)
  plot2a_print
  dev.off()
```

```{r }
m0e <- lmer(RegSuccess  ~ 1 + (1 | PID), data = df, REML = F)
m1e <- lmer(RegSuccess  ~ Intense.z + (1 | PID), data = df, REML = F)
m2e <- lmer(RegSuccess  ~ Distracted + (1 | PID), data = df, REML = F)
m3e <- lmer(RegSuccess  ~ Intense.z + Distracted + (1 | PID), data = df, REML = F)
m4e <- lmer(RegSuccess  ~ Intense.z * Distracted + (1 | PID), data = df, REML = F)
```

```{r Affective Intensity Model Comparisons}
icc(m0e)
anova(m0e, m1e)
anova(m0e, m2e)
anova(m1e, m3e)
anova(m2e, m3e)
anova(m3e, m4e)
```

```{r Affective Intensity Model Outputs, warning=FALSE}
class(m0e) <- "lmerMod"
class(m1e) <- "lmerMod"
class(m2e) <- "lmerMod"
class(m3e) <- "lmerMod"
class(m4e) <- "lmerMod"
stargazer(m0e,m1e,m2e,m3e,m4e,
          type = "text", 
          title= "Affect Predicting Regulation Success", 
          column.labels = c("Null Model", "Model 1", "Model 2", "Model 3", "Model 4"),
          dep.var.labels = "Success", 
          star.cutoffs = c(0.05, 0.01,0.001),
          ci = T,
          notes = "")
```

```{r Simple slopes}
interactions::sim_slopes(model = m4e, pred = Intense.z, modx = Distracted, confint = T)
```


```{r Simple slopes}
sim_slopes <- interactions::sim_slopes(model = m4e, modx = Intense.z, pred = Distracted, confint = T)
```


```{r Testing Differences using T Test}
t.test(x = df$RegSuccess.z[df$Distracted_cat == "Reappraised"],
       y = df$RegSuccess.z[df$Distracted_cat == "Distracted"])
```

```{r Visualizing Regulation Success $ Intensity}
  plot2b_online <- ggplot(data = df, aes(x = df$Intense, y = RegSuccess, linetype = df$Distracted_cat, color = df$Distracted_cat)) +
  stat_smooth(method = "lm",size = 3.0) +
  scale_color_discrete(labels = c("Distraction", "Reappraisal"), name = "Strategy") +
  scale_linetype_manual(values = c("solid", "dotted"), labels = c("Distraction", "Reappraisal"), name = "Strategy") +
  labs(x = "Emotion Intensity",
       y = "Regulation Success") +
  coord_cartesian(ylim=c(0, 4)) +
  theme_classic() +
  theme(axis.title = element_text(size = 24, color = "Black", face = "bold"),
        axis.text.x = element_text(size = 24, color = "Black"),
        axis.text.y = element_text(size = 24, color = "Black"),
        legend.title = element_text(size = 24, face = "bold"),
        legend.position = c(0.200, 0.925)) +
        guides(color = guide_legend(reverse = TRUE),
               linetype = guide_legend(reverse = TRUE))
  plot2b_online
```

```{r Exporting Regulation Success & Intensity Regression}
  tiff(paste0(Plots, "archive/Fig2b_online.tiff"),       
       res = 1200,
       units = "in",
       width = 6.0, 
       height = 6.0)
  plot2b_online
  dev.off()
```

```{r Visualizing Regulation Success $ Intensity}
  plot2b_print <- ggplot(data = df, aes(x = df$Intense, y = df$RegSuccess, linetype = df$Distracted_cat, color = df$Distracted_cat)) +
          stat_smooth(method = "lm", size = 3.0) +
          scale_color_manual(values = c("gray60", "gray20"), labels = c("Distraction", "Reappraisal"), name = "Strategy") +
          scale_linetype_manual(values = c("solid", "dotted"), labels = c("Distraction", "Reappraisal"), name = "Strategy") +
          labs(x = "Emotion Intensity",
               y = "Regulation Success") +
          coord_cartesian(ylim=c(0, 4)) +
          theme_classic() +
          theme(axis.title = element_text(size = 24, color = "Black", face = "bold"),
                axis.text.x = element_text(size = 24, color = "Black"),
                axis.text.y = element_text(size = 24, color = "Black"),
                legend.title = element_text(size = 24, face = "bold"),
                legend.text = element_text(size = 18),
                legend.position = c(0.200, 0.925)) +
            guides(color = guide_legend(reverse = TRUE),
                   linetype = guide_legend(reverse = TRUE))
  plot2b_print
```

```{r Exporting Regulation Success & Intensity Regression}
  tiff(paste0(Plots, "archive/Fig2b_print.tiff"),       
       res = 1200,
       units = "in",
       width = 6.0, 
       height = 6.0)
  plot2b_print
  dev.off()
```
## Using gridExtra to Create a Paneled Image

```{r Making the Paneled image}
# Create text labels as grobs
label_A <- textGrob("A", x = 0, y = 1, hjust = -2.25, vjust = 1.25, gp = gpar(fontsize = 26, fontface = "bold"))
label_B <- textGrob("B", x = 0, y = 1, hjust = -2.25, vjust = 1.25, gp = gpar(fontsize = 26, fontface = "bold"))

# Create a divider with a vertical line
divider <- ggplot() +
  geom_segment(aes(x = 0.5, xend = 0.5, y = 0, yend = 1), size = 1, color = "black") +
  theme_void()

# Arrange the plots with the labels on the side
plot2_online <- grid.arrange(arrangeGrob(label_A, plot2a_online, ncol = 1, heights = c(0.1, 1)),
                             divider,
                             arrangeGrob(label_B, plot2b_online, ncol = 1, heights = c(0.1, 1)),
                             ncol = 3, widths = c(1, 0.05, 1))

# Exporting the images
tiff(paste0(Plots, "Fig2_online.tiff"),       
       res = 1200,
       units = "in",
       width = 12.5, 
       height = 6)
  grid.draw(plot2_online)
  dev.off()

# Arrange the plots with the labels on the side
plot2_print <- grid.arrange(arrangeGrob(label_A, plot2a_print, ncol = 1, heights = c(0.1, 1)),
             divider,
             arrangeGrob(label_B, plot2b_print, ncol = 1, heights = c(0.1, 1)),
             ncol = 3, widths = c(1, 0.05, 1))

# Exporting the images
tiff(paste0(Plots, "Fig2_print.tiff"),       
       res = 1200,
       units = "in",
       width = 12.5, 
       height = 6)
  grid.draw(plot2_print)
  dev.off()
```


## Application of Engagement/Disengagement Strategies with Positive and Negative Emotions During Immediate Recall

You can see the title above; you get the picture. Immediate recall refers to those events that were recalled for the first time *right* after leaving the haunted house, rather than at the follow up some days later.  

```{r Creating a Subset of Data}
df <- subset(df_sub, 
             !is.na(df_sub$Disengage) &
             df_sub$Timing == "ImmReg",
             select = c("Disengage", "PID", "Intense.z", "Intense.pc.z", "Intense.pmean.z", "Timing", "EventNum", "Fear.Before", "Fear.Enjoy", "ERQ_CogReap"))
```

```{r Printing Details}
paste("Number of Participants:", length(unique(df$PID)))
paste("Number of Events:", nrow(distinct(subset(df, select = c("PID", "Timing", "EventNum")))))
paste("Number of Immediate Events:", nrow(distinct(subset(df, df$Timing == "ImmReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Familiar Recall Events:", nrow(distinct(subset(df, df$Timing == "MemReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Novel Recall Events:", nrow(distinct(subset(df, df$Timing == "DelReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Disengagement cases:", length(which(df$Disengage == 1)))
paste("Number of Engagement cases:", length(which(df$Disengage == 0)))
```

```{r Affective Intensity Models}
m0 <- glmer(Disengage ~ 1 + (1 | PID), data = df, family = binomial)
m1 <- glmer(Disengage  ~ Intense.z + (1 | PID), data = df, family = binomial)
m2 <- glmer(Disengage  ~ Intense.pc.z + Intense.pmean.z + (1 | PID), data = df, family = binomial)
mcov <- glmer(Disengage  ~ Intense.z + Fear.Before + Fear.Enjoy + ERQ_CogReap + (1 | PID), data = df, family = binomial)
```

```{r Affective Intensity Model Comparisons}
icc(m0)
anova(m0, m1)
anova(m0, m2)
```

```{r Affective Intensity Model Outputs, warning=FALSE}
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
class(mcov) <- "lmerMod"
stargazer(m0,m1,m2,mcov,
          type = "text", 
          title= "Affect Predicting Regulation Usage", 
          column.labels = c("Null Model", "Model 1", "Model 2"),
          dep.var.labels = "Strategy Usage", 
          star.cutoffs = c(0.05, 0.01,0.001),
          covariate.labels = c("Affective Intensity (z)", "Affective Intensity (Person-Centered, z)", 
                               "Affective Intensity (Person-Mean)", "Fear Expectation" , "Fear Enjoy" , 
                               "ERQ (Reappraisal)" , "Intercept"),
          ci = T,
          notes = "")
```

```{r Calculating Effect Sizes for Affective Intensity Models}
exp(fixef(m1))
exp(-0.350)
exp(0.477)

exp(fixef(mcov))
exp(-0.427)
exp(0.457)
```

## Application of Engagement/Disengagement Strategies with Negative Emotions During Immediate Recall

Now we're specifically going to look at negative emotions in that context ...

```{r Creating a Subset of Data}
df <- subset(df_sub, 
             !is.na(df_sub$Disengage) &
             df_sub$Valence < 0.5 &
             df_sub$Timing == "ImmReg",
             select = c("Disengage", "PID", "Intense.z", "Intense.pc.z", "Intense.pmean.z", "Timing", "EventNum", "Fear.Before", "Fear.Enjoy", "ERQ_CogReap"))
```

```{r Printing Details}
paste("Number of Participants:", length(unique(df$PID)))
paste("Number of Events:", nrow(distinct(subset(df, select = c("PID", "Timing", "EventNum")))))
paste("Number of Immediate Events:", nrow(distinct(subset(df, df$Timing == "ImmReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Familiar Recall Events:", nrow(distinct(subset(df, df$Timing == "MemReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Novel Recall Events:", nrow(distinct(subset(df, df$Timing == "DelReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Disengagement cases:", length(which(df$Disengage == 1)))
paste("Number of Engagement cases:", length(which(df$Disengage == 0)))
```

```{r Affective Intensity Models}
m0 <- glmer(Disengage ~ 1 + (1 | PID), data = df, family = binomial)
m1 <- glmer(Disengage  ~ Intense.z + (1 | PID), data = df, family = binomial)
m2 <- glmer(Disengage  ~ Intense.pc.z + Intense.pmean.z + (1 | PID), data = df, family = binomial)
mcov <- glmer(Disengage  ~ Intense.z + Fear.Before + Fear.Enjoy + ERQ_CogReap + (1 | PID), data = df, family = binomial)
```

```{r Affective Intensity Model Comparisons}
icc(m0)
anova(m0, m1)
anova(m0, m2)
```

```{r Affective Intensity Model Outputs, warning=FALSE}
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
class(mcov) <- "lmerMod"
stargazer(m0,m1,m2,mcov,
          type = "text", 
          title= "Affect Predicting Regulation Usage", 
          column.labels = c("Null Model", "Model 1", "Model 2"),
          dep.var.labels = "Strategy Usage", 
          star.cutoffs = c(0.05, 0.01,0.001),
          covariate.labels = c("Affective Intensity (z)", "Affective Intensity (Person-Centered, z)", 
                               "Affective Intensity (Person-Mean)", "Fear Expectation" , "Fear Enjoy" , 
                               "ERQ (Reappraisal)" , "Intercept"),
          ci = T,
          notes = "")
```

```{r Calculating Effect Sizes for Affective Intensity Models}
exp(fixef(m1))
exp(-0.353)
exp(0.518)

exp(fixef(mcov))
exp(-0.427)
exp(0.516)
```

## Application of Reappraisal/Distract Strategies with Positive and Negative Emotions During Immediate Recall

And we're going to finish off with just reappraisal and distraction in that context. 

```{r Creating a Subset of Data}
df <- subset(df_sub, 
             !is.na(df_sub$Distracted) &
             df_sub$Timing == "ImmReg",
             select = c("Distracted", "PID", "Intense.z", "Intense.pc.z", "Intense.pmean.z", "Timing", "EventNum", "Fear.Before", "Fear.Enjoy", "ERQ_CogReap"))
```

```{r Printing Details}
paste("Number of Participants:", length(unique(df$PID)))
paste("Number of Events:", nrow(distinct(subset(df, select = c("PID", "Timing", "EventNum")))))
paste("Number of Immediate Events:", nrow(distinct(subset(df, df$Timing == "ImmReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Familiar Recall Events:", nrow(distinct(subset(df, df$Timing == "MemReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Novel Recall Events:", nrow(distinct(subset(df, df$Timing == "DelReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Distracted cases:", length(which(df$Distracted == 1)))
paste("Number of Engagement cases:", length(which(df$Distracted == 0)))
```

```{r Affective Intensity Models}
m0 <- glmer(Distracted ~ 1 + (1 | PID), data = df, family = binomial)
m1 <- glmer(Distracted  ~ Intense.z + (1 | PID), data = df, family = binomial)
m2 <- glmer(Distracted  ~ Intense.pc.z + Intense.pmean.z + (1 | PID), data = df, family = binomial)
mcov <- glmer(Distracted  ~ Intense.z + Fear.Before + Fear.Enjoy + ERQ_CogReap + (1 | PID), data = df, family = binomial)
```

```{r Affective Intensity Model Comparisons}
icc(m0)
anova(m0, m1)
anova(m0, m2)
```

```{r Affective Intensity Model Outputs, warning=FALSE}
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
class(mcov) <- "lmerMod"
stargazer(m0,m1,m2,mcov,
          type = "text", 
          title= "Affect Predicting Regulation Usage", 
          column.labels = c("Null Model", "Model 1", "Model 2"),
          dep.var.labels = "Strategy Usage", 
          star.cutoffs = c(0.05, 0.01,0.001),
          covariate.labels = c("Affective Intensity (z)", "Affective Intensity (Person-Centered, z)", 
                               "Affective Intensity (Person-Mean)", "Fear Expectation" , "Fear Enjoy" , 
                               "ERQ (Reappraisal)" , "Intercept"),
          ci = T,
          notes = "")
```

```{r Calculating Effect Sizes for Affective Intensity Models}
exp(fixef(m1))
exp(-0.232)
exp(0.697)

exp(fixef(mcov))
exp(-0.244)
exp(0.749)
```

## Application of Reappraisal/Distract Strategies with Negative Emotions During Immediate Recall

```{r Creating a Subset of Data}
df <- subset(df_sub, 
             !is.na(df_sub$Distracted) &
             df_sub$Valence < 0.5 &
             df_sub$Timing == "ImmReg",
             select = c("Distracted", "PID", "Intense.z", "Intense.pc.z", "Intense.pmean.z", "Timing", "EventNum", "Fear.Before", "Fear.Enjoy", "ERQ_CogReap"))
```

```{r Printing Details}
paste("Number of Participants:", length(unique(df$PID)))
paste("Number of Events:", nrow(distinct(subset(df, select = c("PID", "Timing", "EventNum")))))
paste("Number of Immediate Events:", nrow(distinct(subset(df, df$Timing == "ImmReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Familiar Recall Events:", nrow(distinct(subset(df, df$Timing == "MemReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Novel Recall Events:", nrow(distinct(subset(df, df$Timing == "DelReg", select = c("PID", "Timing", "EventNum")))))
paste("Number of Distracted cases:", length(which(df$Distracted == 1)))
paste("Number of Engagement cases:", length(which(df$Distracted == 0)))
```

```{r Affective Intensity Models}
m0 <- glmer(Distracted ~ 1 + (1 | PID), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa"))
m1 <- glmer(Distracted  ~ Intense.z + (1 | PID), data = df, family = binomial,control = glmerControl(optimizer = "bobyqa"))
m2 <- glmer(Distracted  ~ Intense.pc.z + Intense.pmean.z + (1 | PID), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa"))
mcov <- glmer(Distracted  ~ Intense.z + Fear.Before + Fear.Enjoy + ERQ_CogReap + (1 | PID), data = df, family = binomial, control = glmerControl(optimizer = "bobyqa"))
```

```{r Affective Intensity Model Comparisons}
icc(m0)
anova(m0, m1)
anova(m0, m2)
```

```{r Affective Intensity Model Outputs, warning=FALSE}
class(m0) <- "lmerMod"
class(m1) <- "lmerMod"
class(m2) <- "lmerMod"
class(mcov) <- "lmerMod"
stargazer(m0,m1,m2,mcov,
          type = "text", 
          title= "Affect Predicting Regulation Usage", 
          column.labels = c("Null Model", "Model 1", "Model 2"),
          dep.var.labels = "Strategy Usage", 
          star.cutoffs = c(0.05, 0.01,0.001),
          covariate.labels = c("Affective Intensity (z)", "Affective Intensity (Person-Centered, z)", 
                               "Affective Intensity (Person-Mean)", "Fear Expectation" , "Fear Enjoy" , 
                               "ERQ (Reappraisal)" , "Intercept"),
          ci = T,
          notes = "")
```

```{r Calculating Effect Sizes for Affective Intensity Models}
exp(fixef(m1))
exp(-0.211)
exp(0.739)

exp(fixef(mcov))
exp(-0.261)
exp(0.824)
```